{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.duckdb.duckdb_linker import DuckDBLinker\n",
    "import splink.duckdb.duckdb_comparison_library as cl\n",
    "from splink.duckdb.duckdb_comparison_library import (\n",
    "    exact_match,\n",
    "    levenshtein_at_thresholds,\n",
    "    jaro_winkler_at_thresholds\n",
    ")\n",
    "import splink.duckdb.duckdb_comparison_template_library as ctl\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Label File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel(\"../../Dataset/Febrl1_Data.xlsx\")\n",
    "true_labels = pd.read_excel(\"../../Dataset/True_links.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = true_labels[['Original','Duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels['Duplicate'].nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splink Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage.datasets import load_febrl1\n",
    "\n",
    "febrl1_data = load_febrl1()\n",
    "febrl1_data = febrl1_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rec_id', 'given_name', 'surname', 'street_number', 'address_1',\n",
       "       'address_2', 'suburb', 'postcode', 'state', 'date_of_birth',\n",
       "       'soc_sec_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "febrl1_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "febrl1_data.to_excel('../../Dataset/Input Datasets/Febrl1_Data.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>soc_sec_id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rec-1496-org</td>\n",
       "      <td>mitchell</td>\n",
       "      <td>green</td>\n",
       "      <td>7.0</td>\n",
       "      <td>wallaby place</td>\n",
       "      <td>delmar</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>2119</td>\n",
       "      <td>sa</td>\n",
       "      <td>19560409</td>\n",
       "      <td>1804974</td>\n",
       "      <td>rec-1496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rec-552-dup-3</td>\n",
       "      <td>harley</td>\n",
       "      <td>mccarthy</td>\n",
       "      <td>177.0</td>\n",
       "      <td>pridhamstreet</td>\n",
       "      <td>milton</td>\n",
       "      <td>marsden</td>\n",
       "      <td>3165</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19080419</td>\n",
       "      <td>6089216</td>\n",
       "      <td>rec-552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rec_id given_name   surname  ...  date_of_birth soc_sec_id   cluster\n",
       "0   rec-1496-org   mitchell     green  ...       19560409    1804974  rec-1496\n",
       "1  rec-552-dup-3     harley  mccarthy  ...       19080419    6089216   rec-552\n",
       "\n",
       "[2 rows x 12 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"../../Dataset/Input Datasets/Febrl3_Data.xlsx\", dtype={\"date_of_birth\":str})\n",
    "df[\"cluster\"] = df[\"rec_id\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['rec_id', 'given_name',\n",
    "       'surname', 'street_number', 'address_1', 'address_2', 'suburb',\n",
    "       'postcode', 'state', 'date_of_birth', 'soc_sec_id', 'cluster']]\n",
    "\n",
    "df.to_excel(r'../../Dataset/Input Datasets/Febrl3_Data.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "\"unique_id_column_name\": \"rec_id\",\n",
    "\"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules_to_generate_predictions\": [\n",
    "        \"l.surname = r.surname\",\n",
    "        \"l.postcode = r.postcode\"\n",
    "    ],\n",
    "    \"comparisons\": [\n",
    "        ctl.name_comparison(\"given_name\"),\n",
    "        ctl.name_comparison(\"surname\"),\n",
    "        levenshtein_at_thresholds(\"date_of_birth\",[1,2]),\n",
    "        jaro_winkler_at_thresholds(\"address_1\",[0.9,0.7]),\n",
    "        jaro_winkler_at_thresholds(\"address_2\",[0.9,0.7]),\n",
    "        levenshtein_at_thresholds(\"suburb\",2),\n",
    "        levenshtein_at_thresholds(\"state\",2),\n",
    "        exact_match(\"street_number\", term_frequency_adjustments = True),\n",
    "        exact_match(\"postcode\", term_frequency_adjustments = True)\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----- Estimating u probabilities using random sampling -----\n",
      "\n",
      "Estimated u probabilities using random sampling\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - given_name (no m values are trained).\n",
      "    - surname (no m values are trained).\n",
      "    - date_of_birth (no m values are trained).\n",
      "    - address_1 (no m values are trained).\n",
      "    - address_2 (no m values are trained).\n",
      "    - suburb (no m values are trained).\n",
      "    - state (no m values are trained).\n",
      "    - street_number (no m values are trained).\n",
      "    - postcode (no m values are trained).\n"
     ]
    }
   ],
   "source": [
    "linker = DuckDBLinker(df,settings)\n",
    "linker.estimate_u_using_random_sampling(max_pairs=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.given_name = r.given_name and l.surname = r.surname\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - date_of_birth\n",
      "    - address_1\n",
      "    - address_2\n",
      "    - suburb\n",
      "    - state\n",
      "    - street_number\n",
      "    - postcode\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - given_name\n",
      "    - surname\n",
      "\n",
      "Iteration 1: Largest change in params was -0.476 in the m_probability of address_2, level `Exact match`\n",
      "Iteration 2: Largest change in params was 0.00358 in probability_two_random_records_match\n",
      "Iteration 3: Largest change in params was 3.02e-05 in probability_two_random_records_match\n",
      "\n",
      "EM converged after 3 iterations\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - given_name (no m values are trained).\n",
      "    - surname (no m values are trained).\n",
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.date_of_birth = r.date_of_birth\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - given_name\n",
      "    - surname\n",
      "    - address_1\n",
      "    - address_2\n",
      "    - suburb\n",
      "    - state\n",
      "    - street_number\n",
      "    - postcode\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - date_of_birth\n",
      "\n",
      "Iteration 1: Largest change in params was 0.797 in probability_two_random_records_match\n",
      "Iteration 2: Largest change in params was 0.00683 in probability_two_random_records_match\n",
      "Iteration 3: Largest change in params was 2.2e-05 in probability_two_random_records_match\n",
      "\n",
      "EM converged after 3 iterations\n",
      "\n",
      "Your model is fully trained. All comparisons have at least one estimate for their m and u values\n",
      "Completed iteration 1, root rows count 2\n",
      "Completed iteration 2, root rows count 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rec_id</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>soc_sec_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>tf_street_number</th>\n",
       "      <th>tf_postcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rec-0-org</td>\n",
       "      <td>3419</td>\n",
       "      <td>3419</td>\n",
       "      <td>rec-0-org</td>\n",
       "      <td>jinni</td>\n",
       "      <td>dreyer</td>\n",
       "      <td>11.0</td>\n",
       "      <td>were street</td>\n",
       "      <td>marriott downs</td>\n",
       "      <td>south melbourne</td>\n",
       "      <td>3172</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19420127.0</td>\n",
       "      <td>3787407</td>\n",
       "      <td>rec-0</td>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rec-10-dup-1</td>\n",
       "      <td>4462</td>\n",
       "      <td>4462</td>\n",
       "      <td>rec-10-dup-1</td>\n",
       "      <td>mikhvyla</td>\n",
       "      <td>hannagan</td>\n",
       "      <td>20.0</td>\n",
       "      <td>windradyen street</td>\n",
       "      <td>brentwood vlge</td>\n",
       "      <td>penshurst</td>\n",
       "      <td>2257</td>\n",
       "      <td>vic</td>\n",
       "      <td>19770501.0</td>\n",
       "      <td>1030769</td>\n",
       "      <td>rec-10</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rec-10-dup-1</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>rec-10-dup-2</td>\n",
       "      <td>mikhayla</td>\n",
       "      <td>hannaan</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>brentwoodvlge</td>\n",
       "      <td>penshurst</td>\n",
       "      <td>2257</td>\n",
       "      <td>vic</td>\n",
       "      <td>19770501.0</td>\n",
       "      <td>1030769</td>\n",
       "      <td>rec-10</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rec-10-dup-1</td>\n",
       "      <td>4820</td>\n",
       "      <td>4820</td>\n",
       "      <td>rec-10-org</td>\n",
       "      <td>mikhayla</td>\n",
       "      <td>hannagan</td>\n",
       "      <td>20.0</td>\n",
       "      <td>rupp place</td>\n",
       "      <td>brentwood vlge</td>\n",
       "      <td>penshurst</td>\n",
       "      <td>2257</td>\n",
       "      <td>vic</td>\n",
       "      <td>19770501.0</td>\n",
       "      <td>1030769</td>\n",
       "      <td>rec-10</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rec-100-dup-0</td>\n",
       "      <td>3567</td>\n",
       "      <td>3567</td>\n",
       "      <td>rec-100-dup-0</td>\n",
       "      <td>domenique</td>\n",
       "      <td>paterson</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>merewether pufblic school</td>\n",
       "      <td>greensborough</td>\n",
       "      <td>3138</td>\n",
       "      <td>qld</td>\n",
       "      <td>19161017.0</td>\n",
       "      <td>4975843</td>\n",
       "      <td>rec-100</td>\n",
       "      <td>0.020189</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>rec-1880-dup-0</td>\n",
       "      <td>924</td>\n",
       "      <td>924</td>\n",
       "      <td>rec-1880-dup-2</td>\n",
       "      <td>callum</td>\n",
       "      <td>doody</td>\n",
       "      <td>11.0</td>\n",
       "      <td>emery s treet</td>\n",
       "      <td>texas station</td>\n",
       "      <td>orange</td>\n",
       "      <td>7021</td>\n",
       "      <td>sa</td>\n",
       "      <td>19760824.0</td>\n",
       "      <td>8675169</td>\n",
       "      <td>rec-1880</td>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>rec-526-dup-0</td>\n",
       "      <td>1573</td>\n",
       "      <td>1573</td>\n",
       "      <td>rec-526-dup-2</td>\n",
       "      <td>maya</td>\n",
       "      <td>greqen</td>\n",
       "      <td>20.0</td>\n",
       "      <td>louisa laws on crescent</td>\n",
       "      <td>cargo road</td>\n",
       "      <td>bligh park</td>\n",
       "      <td>2560</td>\n",
       "      <td>wa</td>\n",
       "      <td>19790228.0</td>\n",
       "      <td>5349388</td>\n",
       "      <td>rec-526</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>rec-560-dup-0</td>\n",
       "      <td>3038</td>\n",
       "      <td>3038</td>\n",
       "      <td>rec-560-org</td>\n",
       "      <td>david</td>\n",
       "      <td>sokic</td>\n",
       "      <td>11.0</td>\n",
       "      <td>gurubun close</td>\n",
       "      <td>binnalong</td>\n",
       "      <td>dianella</td>\n",
       "      <td>3042</td>\n",
       "      <td>wa</td>\n",
       "      <td>19771128.0</td>\n",
       "      <td>4848529</td>\n",
       "      <td>rec-560</td>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>rec-1294-org</td>\n",
       "      <td>1709</td>\n",
       "      <td>1709</td>\n",
       "      <td>rec-1294-org</td>\n",
       "      <td>kayden</td>\n",
       "      <td>gao</td>\n",
       "      <td>10.0</td>\n",
       "      <td>kater place</td>\n",
       "      <td>table mountain</td>\n",
       "      <td>clifton</td>\n",
       "      <td>4160</td>\n",
       "      <td>sa</td>\n",
       "      <td>19360515.0</td>\n",
       "      <td>2014515</td>\n",
       "      <td>rec-1294</td>\n",
       "      <td>0.022292</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>rec-1251-org</td>\n",
       "      <td>710</td>\n",
       "      <td>710</td>\n",
       "      <td>rec-1251-org</td>\n",
       "      <td>chloe</td>\n",
       "      <td>crossman</td>\n",
       "      <td>73.0</td>\n",
       "      <td>shenton crescent</td>\n",
       "      <td>sec 209 clifton farm</td>\n",
       "      <td>south melbourne</td>\n",
       "      <td>2324</td>\n",
       "      <td>vic</td>\n",
       "      <td>19680424.0</td>\n",
       "      <td>5536009</td>\n",
       "      <td>rec-1251</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cluster_id  Unnamed: 0.1  Unnamed: 0          rec_id given_name  \\\n",
       "0          rec-0-org          3419        3419       rec-0-org      jinni   \n",
       "1       rec-10-dup-1          4462        4462    rec-10-dup-1   mikhvyla   \n",
       "2       rec-10-dup-1           278         278    rec-10-dup-2   mikhayla   \n",
       "3       rec-10-dup-1          4820        4820      rec-10-org   mikhayla   \n",
       "4      rec-100-dup-0          3567        3567   rec-100-dup-0  domenique   \n",
       "...              ...           ...         ...             ...        ...   \n",
       "4995  rec-1880-dup-0           924         924  rec-1880-dup-2     callum   \n",
       "4996   rec-526-dup-0          1573        1573   rec-526-dup-2       maya   \n",
       "4997   rec-560-dup-0          3038        3038     rec-560-org      david   \n",
       "4998    rec-1294-org          1709        1709    rec-1294-org     kayden   \n",
       "4999    rec-1251-org           710         710    rec-1251-org      chloe   \n",
       "\n",
       "       surname  street_number                address_1  \\\n",
       "0       dreyer           11.0              were street   \n",
       "1     hannagan           20.0        windradyen street   \n",
       "2      hannaan           20.0                      NaN   \n",
       "3     hannagan           20.0               rupp place   \n",
       "4     paterson           15.0                      NaN   \n",
       "...        ...            ...                      ...   \n",
       "4995     doody           11.0            emery s treet   \n",
       "4996    greqen           20.0  louisa laws on crescent   \n",
       "4997     sokic           11.0            gurubun close   \n",
       "4998       gao           10.0              kater place   \n",
       "4999  crossman           73.0         shenton crescent   \n",
       "\n",
       "                      address_2           suburb  postcode state  \\\n",
       "0                marriott downs  south melbourne      3172   nsw   \n",
       "1                brentwood vlge        penshurst      2257   vic   \n",
       "2                 brentwoodvlge        penshurst      2257   vic   \n",
       "3                brentwood vlge        penshurst      2257   vic   \n",
       "4     merewether pufblic school    greensborough      3138   qld   \n",
       "...                         ...              ...       ...   ...   \n",
       "4995              texas station           orange      7021    sa   \n",
       "4996                 cargo road       bligh park      2560    wa   \n",
       "4997                  binnalong         dianella      3042    wa   \n",
       "4998             table mountain          clifton      4160    sa   \n",
       "4999       sec 209 clifton farm  south melbourne      2324   vic   \n",
       "\n",
       "      date_of_birth  soc_sec_id   cluster  tf_street_number  tf_postcode  \n",
       "0        19420127.0     3787407     rec-0          0.023344       0.0010  \n",
       "1        19770501.0     1030769    rec-10          0.013670       0.0018  \n",
       "2        19770501.0     1030769    rec-10          0.013670       0.0018  \n",
       "3        19770501.0     1030769    rec-10          0.013670       0.0018  \n",
       "4        19161017.0     4975843   rec-100          0.020189       0.0012  \n",
       "...             ...         ...       ...               ...          ...  \n",
       "4995     19760824.0     8675169  rec-1880          0.023344       0.0012  \n",
       "4996     19790228.0     5349388   rec-526          0.013670       0.0016  \n",
       "4997     19771128.0     4848529   rec-560          0.023344       0.0012  \n",
       "4998     19360515.0     2014515  rec-1294          0.022292       0.0020  \n",
       "4999     19680424.0     5536009  rec-1251          0.000841       0.0008  \n",
       "\n",
       "[5000 rows x 17 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocking_rule_for_training = \"l.given_name = r.given_name and l.surname = r.surname\"\n",
    "linker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n",
    "\n",
    "blocking_rule_for_training = \"l.date_of_birth = r.date_of_birth\"\n",
    "linker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n",
    "\n",
    "pairwise_predictions = linker.predict()\n",
    "\n",
    "clusters = linker.cluster_pairwise_predictions_at_threshold(pairwise_predictions, 0.75)\n",
    "clusters.as_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vl_convert as vlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = linker.roc_chart_from_labels_column(\"cluster\").spec\n",
    "png_data = vlc.vegalite_to_png(vl_spec=spec, scale=2)\n",
    "with open(\"../../Results/Febrl/roc_auc_chart.png\", \"wb\") as f:\n",
    "    f.write(png_data)\n",
    "\n",
    "spec = linker.precision_recall_chart_from_labels_column(\"cluster\").spec\n",
    "png_data = vlc.vegalite_to_png(vl_spec=spec, scale=2)\n",
    "with open(\"../../Results/Febrl/precision_recall_chart.png\", \"wb\") as f:\n",
    "    f.write(png_data)\n",
    "\n",
    "spec = linker.match_weights_chart().spec\n",
    "png_data = vlc.vegalite_to_png(vl_spec=spec, scale=2)\n",
    "with open(\"../../Results/Febrl/match_weight_graph.png\", \"wb\") as f:\n",
    "    f.write(png_data)\n",
    "\n",
    "spec = linker.m_u_parameters_chart().spec\n",
    "png_data = vlc.vegalite_to_png(vl_spec=spec, scale=2)\n",
    "with open(\"../../Results/Febrl/m_u_parameters.png\", \"wb\") as f:\n",
    "    f.write(png_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'view': {'continuousWidth': 400, 'continuousHeight': 300}},\n",
       " 'vconcat': [{'hconcat': [{'data': {'values': [{'percentile_ex_nulls': 0.9832782745361328,\n",
       "        'percentile_inc_nulls': 0.9837999939918518,\n",
       "        'value_count': 81,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 81.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.9690338373184204,\n",
       "        'percentile_inc_nulls': 0.9700000286102295,\n",
       "        'value_count': 69,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 69.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.9564409852027893,\n",
       "        'percentile_inc_nulls': 0.957800030708313,\n",
       "        'value_count': 61,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 61.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.945293128490448,\n",
       "        'percentile_inc_nulls': 0.9470000267028809,\n",
       "        'value_count': 54,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 54.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.9347646832466125,\n",
       "        'percentile_inc_nulls': 0.9368000030517578,\n",
       "        'value_count': 51,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 51.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.9246490597724915,\n",
       "        'percentile_inc_nulls': 0.9269999861717224,\n",
       "        'value_count': 49,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 49.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.9149463176727295,\n",
       "        'percentile_inc_nulls': 0.9175999760627747,\n",
       "        'value_count': 47,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 47.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.9054500460624695,\n",
       "        'percentile_inc_nulls': 0.9083999991416931,\n",
       "        'value_count': 46,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 46.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.8961601853370667,\n",
       "        'percentile_inc_nulls': 0.899399995803833,\n",
       "        'value_count': 45,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 45.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.8874896764755249,\n",
       "        'percentile_inc_nulls': 0.8910000324249268,\n",
       "        'value_count': 42,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 42.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.8633360862731934,\n",
       "        'percentile_inc_nulls': 0.8675999641418457,\n",
       "        'value_count': 39,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 117.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.8404211401939392,\n",
       "        'percentile_inc_nulls': 0.8453999757766724,\n",
       "        'value_count': 37,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 111.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.833402156829834,\n",
       "        'percentile_inc_nulls': 0.8385999798774719,\n",
       "        'value_count': 34,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 34.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.8201899528503418,\n",
       "        'percentile_inc_nulls': 0.8258000016212463,\n",
       "        'value_count': 32,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 64.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.807390570640564,\n",
       "        'percentile_inc_nulls': 0.8134000301361084,\n",
       "        'value_count': 31,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 62.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.79500412940979,\n",
       "        'percentile_inc_nulls': 0.8014000058174133,\n",
       "        'value_count': 30,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 60.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.7890173196792603,\n",
       "        'percentile_inc_nulls': 0.7955999970436096,\n",
       "        'value_count': 29,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 29.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.7832369804382324,\n",
       "        'percentile_inc_nulls': 0.7900000214576721,\n",
       "        'value_count': 28,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 28.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.7665152549743652,\n",
       "        'percentile_inc_nulls': 0.7738000154495239,\n",
       "        'value_count': 27,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 81.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.7611477971076965,\n",
       "        'percentile_inc_nulls': 0.7685999870300293,\n",
       "        'value_count': 26,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 26.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.7508257627487183,\n",
       "        'percentile_inc_nulls': 0.7585999965667725,\n",
       "        'value_count': 25,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 50.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.73100745677948,\n",
       "        'percentile_inc_nulls': 0.7394000291824341,\n",
       "        'value_count': 24,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 96.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.6977704763412476,\n",
       "        'percentile_inc_nulls': 0.7071999907493591,\n",
       "        'value_count': 23,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 161.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.6750619411468506,\n",
       "        'percentile_inc_nulls': 0.6851999759674072,\n",
       "        'value_count': 22,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 110.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.6490503549575806,\n",
       "        'percentile_inc_nulls': 0.6599999666213989,\n",
       "        'value_count': 21,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 126.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.640792727470398,\n",
       "        'percentile_inc_nulls': 0.6520000100135803,\n",
       "        'value_count': 20,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 40.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.6290255784988403,\n",
       "        'percentile_inc_nulls': 0.6405999660491943,\n",
       "        'value_count': 19,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 57.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.6104459166526794,\n",
       "        'percentile_inc_nulls': 0.6225999593734741,\n",
       "        'value_count': 18,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 90.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.6034269332885742,\n",
       "        'percentile_inc_nulls': 0.6158000230789185,\n",
       "        'value_count': 17,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 34.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.590214729309082,\n",
       "        'percentile_inc_nulls': 0.6029999852180481,\n",
       "        'value_count': 16,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 64.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.5654417872428894,\n",
       "        'percentile_inc_nulls': 0.5789999961853027,\n",
       "        'value_count': 15,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 120.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.5394302606582642,\n",
       "        'percentile_inc_nulls': 0.5537999868392944,\n",
       "        'value_count': 14,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 126.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.52601158618927,\n",
       "        'percentile_inc_nulls': 0.5407999753952026,\n",
       "        'value_count': 13,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 65.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.5185797214508057,\n",
       "        'percentile_inc_nulls': 0.5335999727249146,\n",
       "        'value_count': 12,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 36.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.5049545764923096,\n",
       "        'percentile_inc_nulls': 0.5203999876976013,\n",
       "        'value_count': 11,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 66.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.47811728715896606,\n",
       "        'percentile_inc_nulls': 0.4944000244140625,\n",
       "        'value_count': 10,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 130.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.4465317726135254,\n",
       "        'percentile_inc_nulls': 0.46380001306533813,\n",
       "        'value_count': 9,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 153.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.41680431365966797,\n",
       "        'percentile_inc_nulls': 0.4350000023841858,\n",
       "        'value_count': 8,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 144.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.37778693437576294,\n",
       "        'percentile_inc_nulls': 0.39719998836517334,\n",
       "        'value_count': 7,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 189.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.34310489892959595,\n",
       "        'percentile_inc_nulls': 0.3636000156402588,\n",
       "        'value_count': 6,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 168.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.28736579418182373,\n",
       "        'percentile_inc_nulls': 0.30959999561309814,\n",
       "        'value_count': 5,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 270.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.24442607164382935,\n",
       "        'percentile_inc_nulls': 0.2680000066757202,\n",
       "        'value_count': 4,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 208.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.19673824310302734,\n",
       "        'percentile_inc_nulls': 0.2218000292778015,\n",
       "        'value_count': 3,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 231.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.14471513032913208,\n",
       "        'percentile_inc_nulls': 0.1714000105857849,\n",
       "        'value_count': 2,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 252.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 0.0,\n",
       "        'percentile_inc_nulls': 0.031199991703033447,\n",
       "        'value_count': 1,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 701.0,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'percentile_ex_nulls': 1.0,\n",
       "        'percentile_inc_nulls': 1.0,\n",
       "        'value_count': 81,\n",
       "        'group_name': 'given_name',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 81.0,\n",
       "        'distinct_value_count': 1213}]},\n",
       "     'mark': {'type': 'line', 'interpolate': 'step-after'},\n",
       "     'encoding': {'x': {'type': 'quantitative',\n",
       "       'field': 'percentile_ex_nulls',\n",
       "       'sort': 'descending',\n",
       "       'title': 'Percentile'},\n",
       "      'y': {'type': 'quantitative',\n",
       "       'field': 'value_count',\n",
       "       'title': 'Count of values'},\n",
       "      'tooltip': [{'field': 'value_count', 'type': 'quantitative'},\n",
       "       {'field': 'percentile_ex_nulls', 'type': 'quantitative'},\n",
       "       {'field': 'percentile_inc_nulls', 'type': 'quantitative'},\n",
       "       {'field': 'total_non_null_rows', 'type': 'quantitative'},\n",
       "       {'field': 'total_rows_inc_nulls', 'type': 'quantitative'}]},\n",
       "     'title': {'text': 'Distribution of counts of values in column given_name',\n",
       "      'subtitle': 'In this col, 156 values (3.1%) are null and there are 1213 distinct values'}},\n",
       "    {'data': {'values': [{'value_count': 81,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'joshua',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 69,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'emiily',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 61,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'jack',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 54,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'benjamin',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 51,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'isabella',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 49,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'samuel',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 47,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'thomas',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 46,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'sophie',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 45,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'james',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 42,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'william',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213}]},\n",
       "     'mark': 'bar',\n",
       "     'encoding': {'x': {'type': 'nominal',\n",
       "       'field': 'value',\n",
       "       'sort': '-y',\n",
       "       'title': None},\n",
       "      'y': {'type': 'quantitative',\n",
       "       'field': 'value_count',\n",
       "       'title': 'Value count'},\n",
       "      'tooltip': [{'field': 'value', 'type': 'nominal'},\n",
       "       {'field': 'value_count', 'type': 'quantitative'},\n",
       "       {'field': 'total_non_null_rows', 'type': 'quantitative'},\n",
       "       {'field': 'total_rows_inc_nulls', 'type': 'quantitative'}]},\n",
       "     'title': 'Top 10 values by value count'},\n",
       "    {'data': {'values': [{'value_count': 1,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'braecon',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 1,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'lary',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 1,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'katel byn',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 1,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'shakiroh',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213},\n",
       "       {'value_count': 1,\n",
       "        'group_name': 'given_name',\n",
       "        'value': 'nathab',\n",
       "        'total_non_null_rows': 4844,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1213}]},\n",
       "     'mark': 'bar',\n",
       "     'encoding': {'x': {'type': 'nominal',\n",
       "       'field': 'value',\n",
       "       'sort': '-y',\n",
       "       'title': None},\n",
       "      'y': {'type': 'quantitative',\n",
       "       'field': 'value_count',\n",
       "       'title': 'Value count',\n",
       "       'scale': {'domain': [0, 81]}},\n",
       "      'tooltip': [{'field': 'value', 'type': 'nominal'},\n",
       "       {'field': 'value_count', 'type': 'quantitative'},\n",
       "       {'field': 'total_non_null_rows', 'type': 'quantitative'},\n",
       "       {'field': 'total_rows_inc_nulls', 'type': 'quantitative'}]},\n",
       "     'title': 'Bottom 5 values by value count'}]},\n",
       "  {'hconcat': [{'data': {'values': [{'percentile_ex_nulls': 0.9750050902366638,\n",
       "        'percentile_inc_nulls': 0.9753999710083008,\n",
       "        'value_count': 123,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 123.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.9575289487838745,\n",
       "        'percentile_inc_nulls': 0.9581999778747559,\n",
       "        'value_count': 86,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 86.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.9426945447921753,\n",
       "        'percentile_inc_nulls': 0.9435999989509583,\n",
       "        'value_count': 73,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 73.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.9286730289459229,\n",
       "        'percentile_inc_nulls': 0.9297999739646912,\n",
       "        'value_count': 69,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 69.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.9170899987220764,\n",
       "        'percentile_inc_nulls': 0.91839998960495,\n",
       "        'value_count': 57,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 57.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.9069294929504395,\n",
       "        'percentile_inc_nulls': 0.9083999991416931,\n",
       "        'value_count': 50,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 50.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8973785638809204,\n",
       "        'percentile_inc_nulls': 0.8989999890327454,\n",
       "        'value_count': 47,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 47.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8880308866500854,\n",
       "        'percentile_inc_nulls': 0.8898000121116638,\n",
       "        'value_count': 46,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 46.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8612070679664612,\n",
       "        'percentile_inc_nulls': 0.8633999824523926,\n",
       "        'value_count': 44,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 132.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8530786633491516,\n",
       "        'percentile_inc_nulls': 0.8553999662399292,\n",
       "        'value_count': 40,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 40.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8451534509658813,\n",
       "        'percentile_inc_nulls': 0.847599983215332,\n",
       "        'value_count': 39,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 39.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8374314308166504,\n",
       "        'percentile_inc_nulls': 0.8400000333786011,\n",
       "        'value_count': 38,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 38.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8313350677490234,\n",
       "        'percentile_inc_nulls': 0.8339999914169312,\n",
       "        'value_count': 30,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 30.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8254419565200806,\n",
       "        'percentile_inc_nulls': 0.8281999826431274,\n",
       "        'value_count': 29,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 29.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8197520971298218,\n",
       "        'percentile_inc_nulls': 0.8226000070571899,\n",
       "        'value_count': 28,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 28.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.8032920360565186,\n",
       "        'percentile_inc_nulls': 0.8064000010490417,\n",
       "        'value_count': 27,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 81.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7927250862121582,\n",
       "        'percentile_inc_nulls': 0.7960000038146973,\n",
       "        'value_count': 26,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 52.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7876448035240173,\n",
       "        'percentile_inc_nulls': 0.7910000085830688,\n",
       "        'value_count': 25,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 25.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7827677130699158,\n",
       "        'percentile_inc_nulls': 0.7861999869346619,\n",
       "        'value_count': 24,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 24.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7782970666885376,\n",
       "        'percentile_inc_nulls': 0.7818000316619873,\n",
       "        'value_count': 22,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 22.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7697622776031494,\n",
       "        'percentile_inc_nulls': 0.7734000086784363,\n",
       "        'value_count': 21,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 42.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7620402574539185,\n",
       "        'percentile_inc_nulls': 0.7657999992370605,\n",
       "        'value_count': 19,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 38.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7547246217727661,\n",
       "        'percentile_inc_nulls': 0.7585999965667725,\n",
       "        'value_count': 18,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 36.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.751270055770874,\n",
       "        'percentile_inc_nulls': 0.7552000284194946,\n",
       "        'value_count': 17,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 17.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7382645606994629,\n",
       "        'percentile_inc_nulls': 0.7423999905586243,\n",
       "        'value_count': 16,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 64.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.729120135307312,\n",
       "        'percentile_inc_nulls': 0.7333999872207642,\n",
       "        'value_count': 15,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 45.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7234301567077637,\n",
       "        'percentile_inc_nulls': 0.7278000116348267,\n",
       "        'value_count': 14,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 28.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.7128632068634033,\n",
       "        'percentile_inc_nulls': 0.7174000144004822,\n",
       "        'value_count': 13,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 52.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.6982320547103882,\n",
       "        'percentile_inc_nulls': 0.703000009059906,\n",
       "        'value_count': 12,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 72.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.6870554685592651,\n",
       "        'percentile_inc_nulls': 0.6920000314712524,\n",
       "        'value_count': 11,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 55.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.6687664985656738,\n",
       "        'percentile_inc_nulls': 0.6740000247955322,\n",
       "        'value_count': 10,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 90.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.6504775285720825,\n",
       "        'percentile_inc_nulls': 0.656000018119812,\n",
       "        'value_count': 9,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 90.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.626092255115509,\n",
       "        'percentile_inc_nulls': 0.6319999694824219,\n",
       "        'value_count': 8,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 120.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.5933753252029419,\n",
       "        'percentile_inc_nulls': 0.5997999906539917,\n",
       "        'value_count': 7,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 161.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.5458240509033203,\n",
       "        'percentile_inc_nulls': 0.5529999732971191,\n",
       "        'value_count': 6,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 234.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.4696199893951416,\n",
       "        'percentile_inc_nulls': 0.4779999852180481,\n",
       "        'value_count': 5,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 375.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.3907742500305176,\n",
       "        'percentile_inc_nulls': 0.40039998292922974,\n",
       "        'value_count': 4,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 388.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.3060353398323059,\n",
       "        'percentile_inc_nulls': 0.31699997186660767,\n",
       "        'value_count': 3,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 417.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.21296483278274536,\n",
       "        'percentile_inc_nulls': 0.22539997100830078,\n",
       "        'value_count': 2,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 458.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 0.0,\n",
       "        'percentile_inc_nulls': 0.015799999237060547,\n",
       "        'value_count': 1,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 1048.0,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'percentile_ex_nulls': 1.0,\n",
       "        'percentile_inc_nulls': 1.0,\n",
       "        'value_count': 123,\n",
       "        'group_name': 'surname',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'sum_tokens_in_value_count_group': 123.0,\n",
       "        'distinct_value_count': 1740}]},\n",
       "     'mark': {'type': 'line', 'interpolate': 'step-after'},\n",
       "     'encoding': {'x': {'type': 'quantitative',\n",
       "       'field': 'percentile_ex_nulls',\n",
       "       'sort': 'descending',\n",
       "       'title': 'Percentile'},\n",
       "      'y': {'type': 'quantitative',\n",
       "       'field': 'value_count',\n",
       "       'title': 'Count of values'},\n",
       "      'tooltip': [{'field': 'value_count', 'type': 'quantitative'},\n",
       "       {'field': 'percentile_ex_nulls', 'type': 'quantitative'},\n",
       "       {'field': 'percentile_inc_nulls', 'type': 'quantitative'},\n",
       "       {'field': 'total_non_null_rows', 'type': 'quantitative'},\n",
       "       {'field': 'total_rows_inc_nulls', 'type': 'quantitative'}]},\n",
       "     'title': {'text': 'Distribution of counts of values in column surname',\n",
       "      'subtitle': 'In this col, 79 values (1.6%) are null and there are 1740 distinct values'}},\n",
       "    {'data': {'values': [{'value_count': 123,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'white',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 86,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'clarke',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 73,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'campbell',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 69,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'ryan',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 57,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'green',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 50,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'reid',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 47,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'dixon',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 46,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'nguyen',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 44,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'webb',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 44,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'morrison',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740}]},\n",
       "     'mark': 'bar',\n",
       "     'encoding': {'x': {'type': 'nominal',\n",
       "       'field': 'value',\n",
       "       'sort': '-y',\n",
       "       'title': None},\n",
       "      'y': {'type': 'quantitative',\n",
       "       'field': 'value_count',\n",
       "       'title': 'Value count'},\n",
       "      'tooltip': [{'field': 'value', 'type': 'nominal'},\n",
       "       {'field': 'value_count', 'type': 'quantitative'},\n",
       "       {'field': 'total_non_null_rows', 'type': 'quantitative'},\n",
       "       {'field': 'total_rows_inc_nulls', 'type': 'quantitative'}]},\n",
       "     'title': 'Top 10 values by value count'},\n",
       "    {'data': {'values': [{'value_count': 1,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'hathaway',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 1,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'mccattmhy',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 1,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'pitno',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 1,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'grifefn',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740},\n",
       "       {'value_count': 1,\n",
       "        'group_name': 'surname',\n",
       "        'value': 'daykin',\n",
       "        'total_non_null_rows': 4921,\n",
       "        'total_rows_inc_nulls': 5000,\n",
       "        'distinct_value_count': 1740}]},\n",
       "     'mark': 'bar',\n",
       "     'encoding': {'x': {'type': 'nominal',\n",
       "       'field': 'value',\n",
       "       'sort': '-y',\n",
       "       'title': None},\n",
       "      'y': {'type': 'quantitative',\n",
       "       'field': 'value_count',\n",
       "       'title': 'Value count',\n",
       "       'scale': {'domain': [0, 123]}},\n",
       "      'tooltip': [{'field': 'value', 'type': 'nominal'},\n",
       "       {'field': 'value_count', 'type': 'quantitative'},\n",
       "       {'field': 'total_non_null_rows', 'type': 'quantitative'},\n",
       "       {'field': 'total_rows_inc_nulls', 'type': 'quantitative'}]},\n",
       "     'title': 'Bottom 5 values by value count'}]}],\n",
       " '$schema': 'https://vega.github.io/schema/vega-lite/v4.8.1.json'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = linker.profile_columns(['given_name', 'surname'], top_n=10, bottom_n=5).spec\n",
    "png_data = vlc.vegalite_to_png(vl_spec=c, scale=2)\n",
    "with open(\"../../Results/Febrl/match_weight_graph.png\", \"wb\") as f:\n",
    "    f.write(png_data)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage.preprocessing import clean,value_occurence\n",
    "df['given_name'] = clean(df['given_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       24\n",
       "1        9\n",
       "2       11\n",
       "3       21\n",
       "4        2\n",
       "        ..\n",
       "4995    61\n",
       "4996    42\n",
       "4997     4\n",
       "4998     5\n",
       "4999    23\n",
       "Name: given_name, Length: 5000, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_occurence(df['given_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../Dataset/Electronic_Health_Record.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unique_id'] = [i for i in range(1,len(df)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>address</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jacinto644</td>\n",
       "      <td>Kris249</td>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>888 Hickle Ferry Suite 38</td>\n",
       "      <td>Hampden County</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>42.151961</td>\n",
       "      <td>-72.598959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alva958</td>\n",
       "      <td>Krajcik437</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>1048 Skiles Trailer</td>\n",
       "      <td>Norfolk County</td>\n",
       "      <td>Walpole</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2081.0</td>\n",
       "      <td>42.177370</td>\n",
       "      <td>-71.281353</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jayson808</td>\n",
       "      <td>Fadel536</td>\n",
       "      <td>1992-06-30</td>\n",
       "      <td>1056 Harris Lane Suite 70</td>\n",
       "      <td>Hampden County</td>\n",
       "      <td>Chicopee</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>42.181642</td>\n",
       "      <td>-72.608842</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jimmie93</td>\n",
       "      <td>Harris789</td>\n",
       "      <td>2004-01-09</td>\n",
       "      <td>201 Mitchell Lodge Unit 67</td>\n",
       "      <td>Plymouth County</td>\n",
       "      <td>Pembroke</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.075292</td>\n",
       "      <td>-70.757035</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gregorio366</td>\n",
       "      <td>Auer97</td>\n",
       "      <td>1996-11-15</td>\n",
       "      <td>1050 Lindgren Extension Apt 38</td>\n",
       "      <td>Suffolk County</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2135.0</td>\n",
       "      <td>42.352434</td>\n",
       "      <td>-71.028610</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>Althea11</td>\n",
       "      <td>O'Hara248</td>\n",
       "      <td>1962-08-17</td>\n",
       "      <td>682 Koss Trafficway Apt 65</td>\n",
       "      <td>Norfolk County</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2457.0</td>\n",
       "      <td>42.294175</td>\n",
       "      <td>-71.259364</td>\n",
       "      <td>12348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>Tarah156</td>\n",
       "      <td>Shields502</td>\n",
       "      <td>1918-10-20</td>\n",
       "      <td>308 Huels Grove Apt 18</td>\n",
       "      <td>Middlesex County</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2453.0</td>\n",
       "      <td>42.372663</td>\n",
       "      <td>-71.209053</td>\n",
       "      <td>12349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12349</th>\n",
       "      <td>Penny812</td>\n",
       "      <td>Pacocha935</td>\n",
       "      <td>1918-10-20</td>\n",
       "      <td>349 Breitenberg Walk Suite 26</td>\n",
       "      <td>Middlesex County</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>42.396186</td>\n",
       "      <td>-71.217928</td>\n",
       "      <td>12350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>Cherlyn665</td>\n",
       "      <td>Quitzon246</td>\n",
       "      <td>1918-10-20</td>\n",
       "      <td>237 Miller Avenue</td>\n",
       "      <td>Middlesex County</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>42.366768</td>\n",
       "      <td>-71.196715</td>\n",
       "      <td>12351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12351</th>\n",
       "      <td>Emory494</td>\n",
       "      <td>Turcotte120</td>\n",
       "      <td>1941-02-11</td>\n",
       "      <td>595 Zulauf Parade Suite 56</td>\n",
       "      <td>Barnstable County</td>\n",
       "      <td>Bourne</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>41.721661</td>\n",
       "      <td>-70.587214</td>\n",
       "      <td>12352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12352 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        given_name      surname   birthdate                         address  \\\n",
       "0       Jacinto644      Kris249  2017-08-24       888 Hickle Ferry Suite 38   \n",
       "1          Alva958   Krajcik437  2016-08-01             1048 Skiles Trailer   \n",
       "2        Jayson808     Fadel536  1992-06-30       1056 Harris Lane Suite 70   \n",
       "3         Jimmie93    Harris789  2004-01-09      201 Mitchell Lodge Unit 67   \n",
       "4      Gregorio366       Auer97  1996-11-15  1050 Lindgren Extension Apt 38   \n",
       "...            ...          ...         ...                             ...   \n",
       "12347     Althea11    O'Hara248  1962-08-17      682 Koss Trafficway Apt 65   \n",
       "12348     Tarah156   Shields502  1918-10-20          308 Huels Grove Apt 18   \n",
       "12349     Penny812   Pacocha935  1918-10-20   349 Breitenberg Walk Suite 26   \n",
       "12350   Cherlyn665   Quitzon246  1918-10-20               237 Miller Avenue   \n",
       "12351     Emory494  Turcotte120  1941-02-11      595 Zulauf Parade Suite 56   \n",
       "\n",
       "                  county         city          state     zip   latitude  \\\n",
       "0         Hampden County  Springfield  Massachusetts  1106.0  42.151961   \n",
       "1         Norfolk County      Walpole  Massachusetts  2081.0  42.177370   \n",
       "2         Hampden County     Chicopee  Massachusetts  1020.0  42.181642   \n",
       "3        Plymouth County     Pembroke  Massachusetts     NaN  42.075292   \n",
       "4         Suffolk County       Boston  Massachusetts  2135.0  42.352434   \n",
       "...                  ...          ...            ...     ...        ...   \n",
       "12347     Norfolk County    Wellesley  Massachusetts  2457.0  42.294175   \n",
       "12348   Middlesex County      Waltham  Massachusetts  2453.0  42.372663   \n",
       "12349   Middlesex County      Waltham  Massachusetts  2452.0  42.396186   \n",
       "12350   Middlesex County      Waltham  Massachusetts  2452.0  42.366768   \n",
       "12351  Barnstable County       Bourne  Massachusetts  2532.0  41.721661   \n",
       "\n",
       "       longitude  unique_id  \n",
       "0     -72.598959          1  \n",
       "1     -71.281353          2  \n",
       "2     -72.608842          3  \n",
       "3     -70.757035          4  \n",
       "4     -71.028610          5  \n",
       "...          ...        ...  \n",
       "12347 -71.259364      12348  \n",
       "12348 -71.209053      12349  \n",
       "12349 -71.217928      12350  \n",
       "12350 -71.196715      12351  \n",
       "12351 -70.587214      12352  \n",
       "\n",
       "[12352 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----- Estimating u probabilities using random sampling -----\n",
      "u probability not trained for given_name - Jaro_winkler_similarity >= 0.95 (comparison vector value: 2). This usually means the comparison level was never observed in the training data.\n",
      "u probability not trained for surname - Jaro_winkler_similarity >= 0.95 (comparison vector value: 2). This usually means the comparison level was never observed in the training data.\n",
      "u probability not trained for address - Exact match (comparison vector value: 3). This usually means the comparison level was never observed in the training data.\n",
      "u probability not trained for county - All other comparisons (comparison vector value: 0). This usually means the comparison level was never observed in the training data.\n",
      "u probability not trained for state - Levenshtein <= 2 (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\n",
      "u probability not trained for state - All other comparisons (comparison vector value: 0). This usually means the comparison level was never observed in the training data.\n",
      "\n",
      "Estimated u probabilities using random sampling\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - given_name (some u values are not trained, no m values are trained).\n",
      "    - surname (some u values are not trained, no m values are trained).\n",
      "    - birthdate (no m values are trained).\n",
      "    - address (some u values are not trained, no m values are trained).\n",
      "    - county (some u values are not trained, no m values are trained).\n",
      "    - city (no m values are trained).\n",
      "    - state (some u values are not trained, no m values are trained).\n",
      "    - zip (no m values are trained).\n",
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.given_name = r.given_name and l.surname = r.surname\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - birthdate\n",
      "    - address\n",
      "    - county\n",
      "    - city\n",
      "    - state\n",
      "    - zip\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - given_name\n",
      "    - surname\n",
      "\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 1: Largest change in params was 0.983 in the m_probability of birthdate, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 2: Largest change in params was -0.0309 in the m_probability of address, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 3: Largest change in params was -0.0968 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 4: Largest change in params was -0.147 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 5: Largest change in params was 0.165 in the m_probability of city, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 6: Largest change in params was 0.134 in the m_probability of city, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 7: Largest change in params was -0.0838 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 8: Largest change in params was -0.044 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 9: Largest change in params was 0.0211 in the m_probability of city, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 10: Largest change in params was -0.00962 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 11: Largest change in params was 0.0043 in the m_probability of city, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 12: Largest change in params was 0.0019 in the m_probability of city, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 13: Largest change in params was -0.00084 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 14: Largest change in params was -0.00037 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 15: Largest change in params was 0.000163 in the m_probability of city, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 1 on comparison birthdate not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.7 on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison zip not observed in dataset, unable to train m value\n",
      "Iteration 16: Largest change in params was 7.17e-05 in the m_probability of city, level `All other comparisons`\n",
      "\n",
      "EM converged after 16 iterations\n",
      "m probability not trained for birthdate - Exact match (comparison vector value: 3). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for birthdate - Levenshtein <= 1 (comparison vector value: 2). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for address - Exact match (comparison vector value: 3). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for address - Jaro_winkler_similarity >= 0.7 (comparison vector value: 2). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for county - All other comparisons (comparison vector value: 0). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for city - Levenshtein <= 2 (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for state - Levenshtein <= 2 (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for state - All other comparisons (comparison vector value: 0). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for zip - Exact match (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - given_name (some u values are not trained, no m values are trained).\n",
      "    - surname (some u values are not trained, no m values are trained).\n",
      "    - birthdate (some m values are not trained).\n",
      "    - address (some u values are not trained, some m values are not trained).\n",
      "    - county (some u values are not trained, some m values are not trained).\n",
      "    - city (some m values are not trained).\n",
      "    - state (some u values are not trained, some m values are not trained).\n",
      "    - zip (some m values are not trained).\n",
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.zip = r.zip\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - given_name\n",
      "    - surname\n",
      "    - birthdate\n",
      "    - address\n",
      "    - county\n",
      "    - city\n",
      "    - state\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - zip\n",
      "\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 1: Largest change in params was 0.95 in the m_probability of given_name, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 2: Largest change in params was -0.716 in the m_probability of birthdate, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 3: Largest change in params was 0.213 in probability_two_random_records_match\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 4: Largest change in params was 0.0572 in the m_probability of address, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 5: Largest change in params was 0.317 in the m_probability of address, level `All other comparisons`\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 6: Largest change in params was -0.0213 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 7: Largest change in params was -0.0768 in the m_probability of city, level `Exact match`\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 8: Largest change in params was 0.0814 in probability_two_random_records_match\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 9: Largest change in params was 0.0266 in probability_two_random_records_match\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 10: Largest change in params was 0.00458 in probability_two_random_records_match\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 11: Largest change in params was 0.000684 in probability_two_random_records_match\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison given_name not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Jaro_winkler_similarity >= 0.95 on comparison surname not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Exact match on comparison address not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison county not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison city not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Levenshtein <= 2 on comparison state not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level All other comparisons on comparison state not observed in dataset, unable to train m value\n",
      "Iteration 12: Largest change in params was 0.0001 in probability_two_random_records_match\n",
      "\n",
      "EM converged after 12 iterations\n",
      "m probability not trained for given_name - Jaro_winkler_similarity >= 0.95 (comparison vector value: 2). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for surname - Jaro_winkler_similarity >= 0.95 (comparison vector value: 2). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for address - Exact match (comparison vector value: 3). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for county - All other comparisons (comparison vector value: 0). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for city - Levenshtein <= 2 (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for state - Levenshtein <= 2 (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for state - All other comparisons (comparison vector value: 0). This usually means the comparison level was never observed in the training data.\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - given_name (some u values are not trained, some m values are not trained).\n",
      "    - surname (some u values are not trained, some m values are not trained).\n",
      "    - address (some u values are not trained, some m values are not trained).\n",
      "    - county (some u values are not trained, some m values are not trained).\n",
      "    - city (some m values are not trained).\n",
      "    - state (some u values are not trained, some m values are not trained).\n",
      "    - zip (some m values are not trained).\n",
      "\n",
      " -- WARNING --\n",
      "You have called predict(), but there are some parameter estimates which have neither been estimated or specified in your settings dictionary.  To produce predictions the following untrained trained parameters will use default values.\n",
      "Comparison: 'given_name':\n",
      "    m values not fully trained\n",
      "Comparison: 'given_name':\n",
      "    u values not fully trained\n",
      "Comparison: 'surname':\n",
      "    m values not fully trained\n",
      "Comparison: 'surname':\n",
      "    u values not fully trained\n",
      "Comparison: 'address':\n",
      "    m values not fully trained\n",
      "Comparison: 'address':\n",
      "    u values not fully trained\n",
      "Comparison: 'county':\n",
      "    m values not fully trained\n",
      "Comparison: 'county':\n",
      "    u values not fully trained\n",
      "Comparison: 'city':\n",
      "    m values not fully trained\n",
      "Comparison: 'state':\n",
      "    m values not fully trained\n",
      "Comparison: 'state':\n",
      "    u values not fully trained\n",
      "Comparison: 'zip':\n",
      "    m values not fully trained\n",
      "Completed iteration 1, root rows count 0\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "\"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules_to_generate_predictions\": [\n",
    "\n",
    "        \"l.given_name = r.given_name\",\n",
    "        \"l.surname = r.surname\"\n",
    "        \n",
    "    ],\n",
    "    \"comparisons\": [\n",
    "\n",
    "            ctl.name_comparison(\"given_name\"),\n",
    "            ctl.name_comparison(\"surname\"),\n",
    "            levenshtein_at_thresholds(\"birthdate\",[1,2]),\n",
    "            jaro_winkler_at_thresholds(\"address\",[0.7,0.5]),\n",
    "            jaro_winkler_at_thresholds(\"county\",[0.7,0.5]),\n",
    "            levenshtein_at_thresholds(\"city\",2),\n",
    "            levenshtein_at_thresholds(\"state\",2),\n",
    "            exact_match(\"zip\", term_frequency_adjustments = True)\n",
    "\n",
    "    ],\n",
    "}\n",
    "\n",
    "linker = DuckDBLinker(df,settings)\n",
    "linker.estimate_u_using_random_sampling(max_pairs=1e6)\n",
    "\n",
    "blocking_rule_for_training = \"l.given_name = r.given_name and l.surname = r.surname\"\n",
    "linker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n",
    "\n",
    "blocking_rule_for_training = \"l.zip = r.zip\"\n",
    "linker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n",
    "\n",
    "pairwise_predictions = linker.predict()\n",
    "\n",
    "clusters = linker.cluster_pairwise_predictions_at_threshold(pairwise_predictions, 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>address</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>tf_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Jacinto644</td>\n",
       "      <td>Kris249</td>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>888 Hickle Ferry Suite 38</td>\n",
       "      <td>Hampden County</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>42.151961</td>\n",
       "      <td>-72.598959</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>Cythia210</td>\n",
       "      <td>Reichel38</td>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>211 Effertz Quay</td>\n",
       "      <td>Essex County</td>\n",
       "      <td>Peabody</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>42.559290</td>\n",
       "      <td>-70.931697</td>\n",
       "      <td>19</td>\n",
       "      <td>0.003881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>52</td>\n",
       "      <td>Chung121</td>\n",
       "      <td>Heller342</td>\n",
       "      <td>2015-07-18</td>\n",
       "      <td>618 Harber Annex</td>\n",
       "      <td>Worcester County</td>\n",
       "      <td>Southborough</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.264526</td>\n",
       "      <td>-71.536202</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>53</td>\n",
       "      <td>Maisha241</td>\n",
       "      <td>Fisher429</td>\n",
       "      <td>1955-05-03</td>\n",
       "      <td>873 Hodkiewicz Dam Unit 1</td>\n",
       "      <td>Plymouth County</td>\n",
       "      <td>Bridgewater</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.993499</td>\n",
       "      <td>-70.972448</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>Caroll955</td>\n",
       "      <td>Jacobi462</td>\n",
       "      <td>1990-07-13</td>\n",
       "      <td>234 Schultz Gate</td>\n",
       "      <td>Worcester County</td>\n",
       "      <td>Webster</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>42.002958</td>\n",
       "      <td>-71.844811</td>\n",
       "      <td>65</td>\n",
       "      <td>0.002687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>4004</td>\n",
       "      <td>4003</td>\n",
       "      <td>Bruno518</td>\n",
       "      <td>Dach178</td>\n",
       "      <td>1936-01-04</td>\n",
       "      <td>286 Ryan Knoll Suite 97</td>\n",
       "      <td>Essex County</td>\n",
       "      <td>Peabody</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.562024</td>\n",
       "      <td>-71.020420</td>\n",
       "      <td>4004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>4066</td>\n",
       "      <td>4065</td>\n",
       "      <td>Franklyn361</td>\n",
       "      <td>Langosh790</td>\n",
       "      <td>1946-06-08</td>\n",
       "      <td>502 Hoeger Overpass</td>\n",
       "      <td>Worcester County</td>\n",
       "      <td>Fitchburg</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>42.565027</td>\n",
       "      <td>-71.788796</td>\n",
       "      <td>4066</td>\n",
       "      <td>0.009104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12349</th>\n",
       "      <td>9100</td>\n",
       "      <td>9099</td>\n",
       "      <td>Thad495</td>\n",
       "      <td>Leannon79</td>\n",
       "      <td>1940-02-25</td>\n",
       "      <td>634 Shields Promenade Suite 47</td>\n",
       "      <td>Norfolk County</td>\n",
       "      <td>Bellingham</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>42.057540</td>\n",
       "      <td>-71.472016</td>\n",
       "      <td>9100</td>\n",
       "      <td>0.003433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>2757</td>\n",
       "      <td>2756</td>\n",
       "      <td>Jacinto644</td>\n",
       "      <td>O'Conner199</td>\n",
       "      <td>1916-05-21</td>\n",
       "      <td>846 Parisian Landing Unit 46</td>\n",
       "      <td>Worcester County</td>\n",
       "      <td>Worcester</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>42.244305</td>\n",
       "      <td>-71.835520</td>\n",
       "      <td>2757</td>\n",
       "      <td>0.004328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12351</th>\n",
       "      <td>1611</td>\n",
       "      <td>1610</td>\n",
       "      <td>Napoleon578</td>\n",
       "      <td>Feeney44</td>\n",
       "      <td>1979-12-27</td>\n",
       "      <td>889 Leannon Union</td>\n",
       "      <td>Middlesex County</td>\n",
       "      <td>Acton</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.434618</td>\n",
       "      <td>-71.400598</td>\n",
       "      <td>1611</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12352 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cluster_id  Unnamed: 0   given_name      surname   birthdate  \\\n",
       "0               1           0   Jacinto644      Kris249  2017-08-24   \n",
       "1              19          18    Cythia210    Reichel38  1999-03-03   \n",
       "2              53          52     Chung121    Heller342  2015-07-18   \n",
       "3              54          53    Maisha241    Fisher429  1955-05-03   \n",
       "4              65          64    Caroll955    Jacobi462  1990-07-13   \n",
       "...           ...         ...          ...          ...         ...   \n",
       "12347        4004        4003     Bruno518      Dach178  1936-01-04   \n",
       "12348        4066        4065  Franklyn361   Langosh790  1946-06-08   \n",
       "12349        9100        9099      Thad495    Leannon79  1940-02-25   \n",
       "12350        2757        2756   Jacinto644  O'Conner199  1916-05-21   \n",
       "12351        1611        1610  Napoleon578     Feeney44  1979-12-27   \n",
       "\n",
       "                              address            county          city  \\\n",
       "0           888 Hickle Ferry Suite 38    Hampden County   Springfield   \n",
       "1                    211 Effertz Quay      Essex County       Peabody   \n",
       "2                    618 Harber Annex  Worcester County  Southborough   \n",
       "3           873 Hodkiewicz Dam Unit 1   Plymouth County   Bridgewater   \n",
       "4                    234 Schultz Gate  Worcester County       Webster   \n",
       "...                               ...               ...           ...   \n",
       "12347         286 Ryan Knoll Suite 97      Essex County       Peabody   \n",
       "12348             502 Hoeger Overpass  Worcester County     Fitchburg   \n",
       "12349  634 Shields Promenade Suite 47    Norfolk County    Bellingham   \n",
       "12350    846 Parisian Landing Unit 46  Worcester County     Worcester   \n",
       "12351               889 Leannon Union  Middlesex County         Acton   \n",
       "\n",
       "               state     zip   latitude  longitude  unique_id    tf_zip  \n",
       "0      Massachusetts  1106.0  42.151961 -72.598959          1  0.004478  \n",
       "1      Massachusetts  1960.0  42.559290 -70.931697         19  0.003881  \n",
       "2      Massachusetts     NaN  42.264526 -71.536202         53       NaN  \n",
       "3      Massachusetts     NaN  41.993499 -70.972448         54       NaN  \n",
       "4      Massachusetts  1570.0  42.002958 -71.844811         65  0.002687  \n",
       "...              ...     ...        ...        ...        ...       ...  \n",
       "12347  Massachusetts     NaN  42.562024 -71.020420       4004       NaN  \n",
       "12348  Massachusetts  1420.0  42.565027 -71.788796       4066  0.009104  \n",
       "12349  Massachusetts  2019.0  42.057540 -71.472016       9100  0.003433  \n",
       "12350  Massachusetts  1609.0  42.244305 -71.835520       2757  0.004328  \n",
       "12351  Massachusetts     NaN  42.434618 -71.400598       1611       NaN  \n",
       "\n",
       "[12352 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = clusters.as_pandas_dataframe()\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_excel('result_patients.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusters = dataframe['cluster_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusters = list(Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "list1= Counter(Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "GeocoderUnavailable",
     "evalue": "HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Gosainganj+Lucknow&format=json&limit=1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000021706A3C700>, 'Connection to nominatim.openstreetmap.org timed out. (connect timeout=1)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[39mraise\u001b[39;00m socket\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mgetaddrinfo returns an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mtimeout\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connection.py:179\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x0000021706A3C700>, 'Connection to nominatim.openstreetmap.org timed out. (connect timeout=1)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    441\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    442\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    443\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    444\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    445\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    449\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    450\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    453\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[0;32m    814\u001b[0m     )\n\u001b[1;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    816\u001b[0m         method,\n\u001b[0;32m    817\u001b[0m         url,\n\u001b[0;32m    818\u001b[0m         body,\n\u001b[0;32m    819\u001b[0m         headers,\n\u001b[0;32m    820\u001b[0m         retries,\n\u001b[0;32m    821\u001b[0m         redirect,\n\u001b[0;32m    822\u001b[0m         assert_same_host,\n\u001b[0;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[0;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[0;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[0;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[0;32m    829\u001b[0m     )\n\u001b[0;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[0;32m    814\u001b[0m     )\n\u001b[1;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    816\u001b[0m         method,\n\u001b[0;32m    817\u001b[0m         url,\n\u001b[0;32m    818\u001b[0m         body,\n\u001b[0;32m    819\u001b[0m         headers,\n\u001b[0;32m    820\u001b[0m         retries,\n\u001b[0;32m    821\u001b[0m         redirect,\n\u001b[0;32m    822\u001b[0m         assert_same_host,\n\u001b[0;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[0;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[0;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[0;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[0;32m    829\u001b[0m     )\n\u001b[0;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    789\u001b[0m )\n\u001b[0;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Gosainganj+Lucknow&format=json&limit=1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000021706A3C700>, 'Connection to nominatim.openstreetmap.org timed out. (connect timeout=1)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\adapters.py:457\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mget(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:542\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 542\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\adapters.py:507\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 507\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectTimeout(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ResponseError):\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Gosainganj+Lucknow&format=json&limit=1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000021706A3C700>, 'Connection to nominatim.openstreetmap.org timed out. (connect timeout=1)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mGeocoderUnavailable\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 29\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loc \u001b[39m=\u001b[39m Nominatim(user_agent \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmyGeocode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# entering the location name\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m getLoc \u001b[39m=\u001b[39m loc\u001b[39m.\u001b[39;49mgeocode(\u001b[39m\"\u001b[39;49m\u001b[39mGosainganj Lucknow\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# # printing address\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(getLoc\u001b[39m.\u001b[39maddress)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\geocoders\\nominatim.py:297\u001b[0m, in \u001b[0;36mNominatim.geocode\u001b[1;34m(self, query, exactly_one, timeout, limit, addressdetails, language, geometry, extratags, country_codes, viewbox, bounded, featuretype, namedetails)\u001b[0m\n\u001b[0;32m    295\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.geocode: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, url)\n\u001b[0;32m    296\u001b[0m callback \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_json, exactly_one\u001b[39m=\u001b[39mexactly_one)\n\u001b[1;32m--> 297\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_geocoder(url, callback, timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\geocoders\\base.py:368\u001b[0m, in \u001b[0;36mGeocoder._call_geocoder\u001b[1;34m(self, url, callback, timeout, is_json, headers)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[39mif\u001b[39;00m is_json:\n\u001b[1;32m--> 368\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madapter\u001b[39m.\u001b[39;49mget_json(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mreq_headers)\n\u001b[0;32m    369\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter\u001b[39m.\u001b[39mget_text(url, timeout\u001b[39m=\u001b[39mtimeout, headers\u001b[39m=\u001b[39mreq_headers)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\adapters.py:447\u001b[0m, in \u001b[0;36mRequestsAdapter.get_json\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_json\u001b[39m(\u001b[39mself\u001b[39m, url, \u001b[39m*\u001b[39m, timeout, headers):\n\u001b[1;32m--> 447\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    448\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m         \u001b[39mreturn\u001b[39;00m resp\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\adapters.py:469\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[39mraise\u001b[39;00m GeocoderServiceError(message)\n\u001b[0;32m    468\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 469\u001b[0m         \u001b[39mraise\u001b[39;00m GeocoderUnavailable(message)\n\u001b[0;32m    470\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(error, requests\u001b[39m.\u001b[39mTimeout):\n\u001b[0;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m GeocoderTimedOut(\u001b[39m\"\u001b[39m\u001b[39mService timed out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mGeocoderUnavailable\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Gosainganj+Lucknow&format=json&limit=1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000021706A3C700>, 'Connection to nominatim.openstreetmap.org timed out. (connect timeout=1)'))"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    " \n",
    "# calling the Nominatim tool\n",
    "loc = Nominatim(user_agent = 'myGeocode')\n",
    " \n",
    "# entering the location name\n",
    "getLoc = loc.geocode(\"Gosainganj Lucknow\")\n",
    "\n",
    "# # printing address\n",
    "print(getLoc.address)\n",
    " \n",
    "# # printing latitude and longitude\n",
    "print(\"Latitude = \", getLoc.latitude, \"\\n\")\n",
    "print(\"Longitude = \", getLoc.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('..\\..\\Dataset\\Input Datasets\\EHR_Deduplication.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'given_name', 'surname', 'birthdate', 'address', 'county',\n",
       "       'city', 'state', 'zip', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas \n",
    "from shapely.geometry import Point\n",
    "\n",
    "latitude = dataframe['latitude'].to_list()\n",
    "longitude = dataframe['longitude'].to_list()\n",
    "coordinates = pd.DataFrame(list(zip(latitude,longitude)))\n",
    "\n",
    "coordinates = coordinates.apply(lambda x: Point(x))\n",
    "\n",
    "df = geopandas.tools.reverse_geocode(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()\n",
    "c.execute(\"\"\" CREATE TABLE employees (first text, last text, pay integer)\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "c.execute(\"Insert into employees('Mary','Schafer', 70000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1ce46d6d110>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.execute(\"insert into employees values ('May','Schafer', 7000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(\"Select * from employees WHERE last = 'Schafer'\")\n",
    "print(c.fetchall())\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.point import Point\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = Nominatim(user_agent='myGeocoder')\n",
    "\n",
    "def reverse_geocoding(lat,lon):\n",
    "\n",
    "    location = locator.reverse(Point(lat, lon))\n",
    "    return location.raw['display_name']\n",
    "\n",
    "\n",
    "dataframe['address_complete'] = np.vectorize(reverse_geocoding)(dataframe['latitude'], dataframe['longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     23;27, Maplecrest Circle, Sandy Hill, Chicopee...\n",
       "1     94, Elm Street, Medfield, Norfolk County, Mass...\n",
       "2     Solenis Chemical, Grattan Street, North Chicop...\n",
       "3     Camp Wing, Union Street, Ashdod, Duxbury, Plym...\n",
       "4     Ted Williams Tunnel, Seaport, South Boston, Bo...\n",
       "5     Vermont Route 112, Halifax, Windham County, Ve...\n",
       "6     27-29, West Sorrento Street, Barry's Corner, A...\n",
       "7     3, Greenacre Road, South Hadley, Hampshire Cou...\n",
       "8     Walpole Sportsman Association, Deerfield Drive...\n",
       "9     76, Stearns Square, Metro Center, Springfield,...\n",
       "10    63, Shawmut Avenue, Middlesex Village, Lowell,...\n",
       "11    86, Albro Avenue, Dartmouth, Bristol County, M...\n",
       "12    544, Quinobequin Road, Waban, Newton, Middlese...\n",
       "13    3, Haslet Street, Roslindale, Boston, Suffolk ...\n",
       "14    56, Wyllis Avenue, West Everett, Everett, Midd...\n",
       "15    6, Gates Street, Framingham, Middlesex County,...\n",
       "16    9, Turner Street, Houghs Neck, Quincy, Norfolk...\n",
       "17    3, Twilight Drive, Foxborough, Norfolk County,...\n",
       "18    Stromin' Into Fitness, 101R, High Street, Danv...\n",
       "19    Rockdale, New Bedford, Bristol County, Massach...\n",
       "20    Rumney Marsh Restoration, Saugus, Essex County...\n",
       "21    21, Storey Drive, Lincoln, Middlesex County, M...\n",
       "22    46, Indian Head Road, Framingham, Middlesex Co...\n",
       "23    18, Esquire Drive, Peabody, Essex County, Mass...\n",
       "24    280, Sportsmen's Trail, Whitman, Plymouth Coun...\n",
       "25    551, Mount Blue Street, Norwell, Plymouth Coun...\n",
       "26    Pine Banks Fields, 1087, Main Street, Windsor ...\n",
       "27    41, Parkhurst Road, Chelmsford, Middlesex Coun...\n",
       "28    17, Churchill Road, Winchester Highlands, Winc...\n",
       "29    106, Elm Street, North Reading, Middlesex Coun...\n",
       "30    313, New West Townsend Road, Lunenburg, Worces...\n",
       "31    Boston, Suffolk County, Massachusetts, United ...\n",
       "32    Mcclelland Farm Road, East Deerfield, Deerfiel...\n",
       "33    Landlocked Forest, Yellow Loop, Burlington, Mi...\n",
       "34    1, Old Sandwich Road, Plymouth, Plymouth Count...\n",
       "35    Cross Road, Dartmouth, Bristol County, Massach...\n",
       "36    Lowell Dracut Tyngsborough State Forest, Gumpu...\n",
       "37    66, Hawkes Street, Saugus, Essex County, Massa...\n",
       "38    3, Raymond Road, South Sudbury, Sudbury, Middl...\n",
       "39    41, Shearer Road, Colrain, Franklin County, Ma...\n",
       "40    Water Supply Land, Station Road, Amherst, Hamp...\n",
       "41    Snow Hill Reservation, Hales Hollow Road, Dove...\n",
       "42    6, Innis Street, Lynnhurst, Saugus, Essex Coun...\n",
       "43    Plum Island Airport, 24, Plum Island Turnpike,...\n",
       "44    242, East Border Road, Medford, Middlesex Coun...\n",
       "45    Nixon Trail, Sudbury, Middlesex County, Massac...\n",
       "46    12, Cranberry Lane, Hingham, Plymouth County, ...\n",
       "47    Twin Pond Trail, Rockland, Plymouth County, Ma...\n",
       "48    18, Raymond Street, Lower Allston, Allston, Bo...\n",
       "49    78, Country Way, Taunton, Bristol County, Mass...\n",
       "Name: address_complete, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['address_complete']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GauravS15\\AppData\\Local\\Temp\\ipykernel_19684\\24518051.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['state'][i] = location.raw['address']['state']\n",
      "C:\\Users\\GauravS15\\AppData\\Local\\Temp\\ipykernel_19684\\24518051.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['county'][i] = location.raw['address']['county']\n",
      "C:\\Users\\GauravS15\\AppData\\Local\\Temp\\ipykernel_19684\\24518051.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['city'][i] = location.raw['address']['city']\n"
     ]
    }
   ],
   "source": [
    "locator = Nominatim(user_agent='myGeocoder')\n",
    "latitude = dataframe['latitude'].to_list()\n",
    "longitude = dataframe['longitude'].to_list()\n",
    "\n",
    "coordinates = list(zip(latitude,longitude))\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(coordinates)):\n",
    "\n",
    "    location = locator.reverse(coordinates[i])\n",
    "    if 'state' in location.raw['address']:\n",
    "        dataframe['state'][i] = location.raw['address']['state'] \n",
    "\n",
    "    if 'postcode' in location.raw['address']:\n",
    "        dataframe['zip'][i] = location.raw['address']['postcode']\n",
    "    \n",
    "    if 'county' in location.raw['address']:\n",
    "        dataframe['county'][i] = location.raw['address']['county']\n",
    "    \n",
    "    if 'city' in location.raw['address']:\n",
    "        dataframe['city'][i] = location.raw['address']['city']\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ehr_df= pd.read_csv(r'C:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Dataset\\patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_ = list(ehr_df.columns)\n",
    "for col in range(0,len(columns_)):\n",
    "    columns_[col] = columns_[col].lower()\n",
    "\n",
    "ehr_df.columns = columns_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_df = ehr_df[['first','last','birthdate','address','county','city','state','zip','lat','lon']]\n",
    "columns_ehr = ['given_name','surname','birthdate','address','county','city','state','zip','latitude','longitude']\n",
    "ehr_df.columns = columns_ehr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_df.to_excel('../../Dataset/Electronic_Health_Record.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>address</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jacinto644</td>\n",
       "      <td>Kris249</td>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>888 Hickle Ferry Suite 38</td>\n",
       "      <td>Hampden County</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>42.151961</td>\n",
       "      <td>-72.598959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alva958</td>\n",
       "      <td>Krajcik437</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>1048 Skiles Trailer</td>\n",
       "      <td>Norfolk County</td>\n",
       "      <td>Walpole</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2081.0</td>\n",
       "      <td>42.177370</td>\n",
       "      <td>-71.281353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jayson808</td>\n",
       "      <td>Fadel536</td>\n",
       "      <td>1992-06-30</td>\n",
       "      <td>1056 Harris Lane Suite 70</td>\n",
       "      <td>Hampden County</td>\n",
       "      <td>Chicopee</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>42.181642</td>\n",
       "      <td>-72.608842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jimmie93</td>\n",
       "      <td>Harris789</td>\n",
       "      <td>2004-01-09</td>\n",
       "      <td>201 Mitchell Lodge Unit 67</td>\n",
       "      <td>Plymouth County</td>\n",
       "      <td>Pembroke</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.075292</td>\n",
       "      <td>-70.757035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gregorio366</td>\n",
       "      <td>Auer97</td>\n",
       "      <td>1996-11-15</td>\n",
       "      <td>1050 Lindgren Extension Apt 38</td>\n",
       "      <td>Suffolk County</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2135.0</td>\n",
       "      <td>42.352434</td>\n",
       "      <td>-71.028610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>Althea11</td>\n",
       "      <td>O'Hara248</td>\n",
       "      <td>1962-08-17</td>\n",
       "      <td>682 Koss Trafficway Apt 65</td>\n",
       "      <td>Norfolk County</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2457.0</td>\n",
       "      <td>42.294175</td>\n",
       "      <td>-71.259364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>Tarah156</td>\n",
       "      <td>Shields502</td>\n",
       "      <td>1918-10-20</td>\n",
       "      <td>308 Huels Grove Apt 18</td>\n",
       "      <td>Middlesex County</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2453.0</td>\n",
       "      <td>42.372663</td>\n",
       "      <td>-71.209053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12349</th>\n",
       "      <td>Penny812</td>\n",
       "      <td>Pacocha935</td>\n",
       "      <td>1918-10-20</td>\n",
       "      <td>349 Breitenberg Walk Suite 26</td>\n",
       "      <td>Middlesex County</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>42.396186</td>\n",
       "      <td>-71.217928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>Cherlyn665</td>\n",
       "      <td>Quitzon246</td>\n",
       "      <td>1918-10-20</td>\n",
       "      <td>237 Miller Avenue</td>\n",
       "      <td>Middlesex County</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>42.366768</td>\n",
       "      <td>-71.196715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12351</th>\n",
       "      <td>Emory494</td>\n",
       "      <td>Turcotte120</td>\n",
       "      <td>1941-02-11</td>\n",
       "      <td>595 Zulauf Parade Suite 56</td>\n",
       "      <td>Barnstable County</td>\n",
       "      <td>Bourne</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>41.721661</td>\n",
       "      <td>-70.587214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12352 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        given_name      surname   birthdate                         address  \\\n",
       "0       Jacinto644      Kris249  2017-08-24       888 Hickle Ferry Suite 38   \n",
       "1          Alva958   Krajcik437  2016-08-01             1048 Skiles Trailer   \n",
       "2        Jayson808     Fadel536  1992-06-30       1056 Harris Lane Suite 70   \n",
       "3         Jimmie93    Harris789  2004-01-09      201 Mitchell Lodge Unit 67   \n",
       "4      Gregorio366       Auer97  1996-11-15  1050 Lindgren Extension Apt 38   \n",
       "...            ...          ...         ...                             ...   \n",
       "12347     Althea11    O'Hara248  1962-08-17      682 Koss Trafficway Apt 65   \n",
       "12348     Tarah156   Shields502  1918-10-20          308 Huels Grove Apt 18   \n",
       "12349     Penny812   Pacocha935  1918-10-20   349 Breitenberg Walk Suite 26   \n",
       "12350   Cherlyn665   Quitzon246  1918-10-20               237 Miller Avenue   \n",
       "12351     Emory494  Turcotte120  1941-02-11      595 Zulauf Parade Suite 56   \n",
       "\n",
       "                  county         city          state     zip   latitude  \\\n",
       "0         Hampden County  Springfield  Massachusetts  1106.0  42.151961   \n",
       "1         Norfolk County      Walpole  Massachusetts  2081.0  42.177370   \n",
       "2         Hampden County     Chicopee  Massachusetts  1020.0  42.181642   \n",
       "3        Plymouth County     Pembroke  Massachusetts     NaN  42.075292   \n",
       "4         Suffolk County       Boston  Massachusetts  2135.0  42.352434   \n",
       "...                  ...          ...            ...     ...        ...   \n",
       "12347     Norfolk County    Wellesley  Massachusetts  2457.0  42.294175   \n",
       "12348   Middlesex County      Waltham  Massachusetts  2453.0  42.372663   \n",
       "12349   Middlesex County      Waltham  Massachusetts  2452.0  42.396186   \n",
       "12350   Middlesex County      Waltham  Massachusetts  2452.0  42.366768   \n",
       "12351  Barnstable County       Bourne  Massachusetts  2532.0  41.721661   \n",
       "\n",
       "       longitude  \n",
       "0     -72.598959  \n",
       "1     -71.281353  \n",
       "2     -72.608842  \n",
       "3     -70.757035  \n",
       "4     -71.028610  \n",
       "...          ...  \n",
       "12347 -71.259364  \n",
       "12348 -71.209053  \n",
       "12349 -71.217928  \n",
       "12350 -71.196715  \n",
       "12351 -70.587214  \n",
       "\n",
       "[12352 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ehr_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splink Data Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage import datasets\n",
    "\n",
    "dfA,dfB = datasets.load_febrl4()\n",
    "dfA = dfA.reset_index()\n",
    "dfB = dfB.reset_index()\n",
    "dfA[\"cluster\"] = dfA[\"rec_id\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\n",
    "dfB[\"cluster\"] = dfB[\"rec_id\"].apply(lambda x: \"-\".join(x.split('-')[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA.to_excel('Febrl_Data4A.xlsx', index=False)\n",
    "dfB.to_excel('Febrl_Data4B.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA['unique_id'] = [x for x in range(1,len(dfA)+1)]\n",
    "dfB['unique_id'] = [x for x in range(1,len(dfB)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [dfA,dfB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.duckdb.duckdb_linker import DuckDBLinker\n",
    "\n",
    "basic_settings = {\n",
    "    \"unique_id_column_name\": \"rec_id\",\n",
    "    \"link_type\": \"link_only\",\n",
    "    # NB as we are linking one-one, we know the probability that a random pair will be a match\n",
    "    # hence we could set:\n",
    "    # \"probability_two_random_records_match\": 1/5000,\n",
    "    # however we will not specify this here, as we will use this as a check that\n",
    "    # our estimation procedure returns something sensible\n",
    "}\n",
    "\n",
    "linker = DuckDBLinker(dfs, basic_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_profile = list(dfs[0].columns)\n",
    "cols_to_profile = [col for col in cols_to_profile if col not in (\"rec_id\", \"cluster\")]\n",
    "linker.profile_columns(cols_to_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-b18f2c30d7c548bf8ac9a244914be57e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-b18f2c30d7c548bf8ac9a244914be57e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-b18f2c30d7c548bf8ac9a244914be57e\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\", \"width\": 450, \"height\": {\"step\": 20}, \"title\": {\"text\": \"Count of Additional Comparisons Generated by Each Blocking Rule\", \"subtitle\": \"(Counts exclude comparisons already generated by previous rules)\"}, \"data\": {\"values\": [{\"row_count\": 2574, \"rule\": \"l.given_name = r.given_name AND l.surname = r.surname\", \"cumulative_rows\": 2574, \"cartesian\": 25000000, \"reduction_ratio\": \"The rolling reduction ratio with your given blocking rule(s) is 0.999897. This represents the reduction in the total number of comparisons due to your rule(s).\", \"start\": 0}, {\"row_count\": 3028, \"rule\": \"l.date_of_birth = r.date_of_birth\", \"cumulative_rows\": 5602, \"cartesian\": 25000000, \"reduction_ratio\": \"The rolling reduction ratio with your given blocking rule(s) is 0.999776. This represents the reduction in the total number of comparisons due to your rule(s).\", \"start\": 2574}, {\"row_count\": 261, \"rule\": \"l.soc_sec_id = r.soc_sec_id\", \"cumulative_rows\": 5863, \"cartesian\": 25000000, \"reduction_ratio\": \"The rolling reduction ratio with your given blocking rule(s) is 0.999765. This represents the reduction in the total number of comparisons due to your rule(s).\", \"start\": 5602}, {\"row_count\": 1592, \"rule\": \"l.state = r.state AND l.address_1 = r.address_1\", \"cumulative_rows\": 7455, \"cartesian\": 25000000, \"reduction_ratio\": \"The rolling reduction ratio with your given blocking rule(s) is 0.999702. This represents the reduction in the total number of comparisons due to your rule(s).\", \"start\": 5863}, {\"row_count\": 83, \"rule\": \"l.street_number = r.street_number AND l.address_1 = r.address_1\", \"cumulative_rows\": 7538, \"cartesian\": 25000000, \"reduction_ratio\": \"The rolling reduction ratio with your given blocking rule(s) is 0.999698. This represents the reduction in the total number of comparisons due to your rule(s).\", \"start\": 7455}, {\"row_count\": 24393, \"rule\": \"l.postcode = r.postcode\", \"cumulative_rows\": 31931, \"cartesian\": 25000000, \"reduction_ratio\": \"The rolling reduction ratio with your given blocking rule(s) is 0.998723. This represents the reduction in the total number of comparisons due to your rule(s).\", \"start\": 7538}]}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"title\": \"Comparisons Generated by Rule(s)\", \"field\": \"start\", \"type\": \"quantitative\"}, \"x2\": {\"field\": \"cumulative_rows\"}, \"y\": {\"field\": \"rule\", \"title\": \"SQL Blocking Rule\", \"sort\": [\"-x2\"]}, \"color\": {\"field\": \"rule\", \"legend\": null, \"scale\": {\"scheme\": \"category20c\"}}, \"order\": {\"field\": \"cumulative_rows\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"rule\", \"title\": \"SQL Condition\"}, {\"type\": \"quantitative\", \"field\": \"row_count\", \"title\": \"Comparisons Generated\", \"format\": \",\"}, {\"type\": \"quantitative\", \"field\": \"cumulative_rows\", \"title\": \"Cumulative Comparisons\", \"format\": \",\"}, {\"type\": \"quantitative\", \"field\": \"cartesian\", \"title\": \"Cartesian Product of Input Data\", \"format\": \",\"}, {\"type\": \"nominal\", \"field\": \"reduction_ratio\", \"title\": \"Reduction Ratio (cumulative rows/cartesian product)\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "<splink.charts.VegaliteNoValidate at 0x225c83e9b50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocking_rules = [\n",
    "    \"l.given_name = r.given_name AND l.surname = r.surname\",\n",
    "    \"l.date_of_birth = r.date_of_birth\",\n",
    "    \"l.soc_sec_id = r.soc_sec_id\",\n",
    "    \"l.state = r.state AND l.address_1 = r.address_1\",\n",
    "    \"l.street_number = r.street_number AND l.address_1 = r.address_1\",\n",
    "    \"l.postcode = r.postcode\",\n",
    "]\n",
    "linker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splink.duckdb.duckdb_comparison_level_library as cll\n",
    "import splink.duckdb.duckdb_comparison_library as cl\n",
    "import splink.duckdb.duckdb_comparison_template_library as ctl\n",
    "\n",
    "# the simple model only considers a few columns, and only two comparison levels for each\n",
    "simple_model_settings = {\n",
    "    **basic_settings,\n",
    "    \"blocking_rules_to_generate_predictions\": blocking_rules,\n",
    "    \"comparisons\": [\n",
    "        cl.exact_match(\"given_name\", term_frequency_adjustments=True),\n",
    "        cl.exact_match(\"surname\", term_frequency_adjustments=True),\n",
    "        cl.exact_match(\"street_number\", term_frequency_adjustments=True),\n",
    "    ],\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "}\n",
    "# the detailed model considers more columns, using the information we saw in the exploratory phase\n",
    "# we also include further comparison levels to account for typos and other differences\n",
    "detailed_model_settings = {\n",
    "    **basic_settings,\n",
    "    \"blocking_rules_to_generate_predictions\": blocking_rules,\n",
    "    \"comparisons\": [\n",
    "        ctl.name_comparison(\"given_name\", term_frequency_adjustments_name=True),\n",
    "        ctl.name_comparison(\"surname\", term_frequency_adjustments_name=True),\n",
    "        cl.levenshtein_at_thresholds(\"date_of_birth\", [1, 2]),\n",
    "        cl.levenshtein_at_thresholds(\"soc_sec_id\", [1, 2]),\n",
    "        cl.exact_match(\"street_number\", term_frequency_adjustments=True),\n",
    "        cl.levenshtein_at_thresholds(\"postcode\", [1, 2], term_frequency_adjustments=True),\n",
    "        # we don't consider further location columns as they will be strongly correlated with postcode\n",
    "    ],\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "}\n",
    "\n",
    "\n",
    "linker_simple = DuckDBLinker(dfs, simple_model_settings)\n",
    "linker_detailed = DuckDBLinker(dfs, detailed_model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Probability two random records match is estimated to be  0.000238.\n",
      "This means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match.  With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs\n"
     ]
    }
   ],
   "source": [
    "deterministic_rules = [\n",
    "    \"l.soc_sec_id = r.soc_sec_id\",\n",
    "    \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\",\n",
    "]\n",
    "\n",
    "linker_detailed.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----- Estimating u probabilities using random sampling -----\n",
      "\n",
      "Estimated u probabilities using random sampling\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - given_name (no m values are trained).\n",
      "    - surname (no m values are trained).\n",
      "    - date_of_birth (no m values are trained).\n",
      "    - soc_sec_id (no m values are trained).\n",
      "    - street_number (no m values are trained).\n",
      "    - postcode (no m values are trained).\n"
     ]
    }
   ],
   "source": [
    "linker_detailed.estimate_u_using_random_sampling(max_pairs=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.date_of_birth = r.date_of_birth\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "\n"
     ]
    },
    {
     "ename": "SplinkException",
     "evalue": "Error executing the following sql for table `__splink__m_u_counts`(__splink__m_u_counts_0aaeffeca):\n\n        CREATE TABLE __splink__m_u_counts_0aaeffeca\n        AS\n        (WITH __splink__df_comparison_vectors as (select * from __splink__df_comparison_vectors_9a9886873), \n__splink__df_match_weight_parts as (\n    select \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_comparison_vectors\n    ), \n__splink__df_predict as (\n    select\n    log2(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END) as match_weight,\n    CASE WHEN  THEN 1.0 ELSE ((CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END)/(1+(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END))) END as match_probability,\n    \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_match_weight_parts\n    \n    ) \n    select 0 as comparison_vector_value,\n           avg(match_probability) as m_count,\n           avg(1-match_probability) as u_count,\n           '_probability_two_random_records_match' as output_column_name\n    from __splink__df_predict\n    )\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:581\u001b[0m, in \u001b[0;36mLinker._log_and_run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sql_execution(final_sql, templated_name, physical_name)\n\u001b[0;32m    582\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\duckdb\\duckdb_linker.py:181\u001b[0m, in \u001b[0;36mDuckDBLinker._run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_sql_execution\u001b[39m(\u001b[39mself\u001b[39m, final_sql, templated_name, physical_name):\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_con\u001b[39m.\u001b[39;49mexecute(final_sql)\n",
      "\u001b[1;31mParserException\u001b[0m: Parser Error: syntax error at or near \"THEN\"\nLINE 11:     log2(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END) as match_weight,\n    CASE WHEN  THEN 1.0 ELSE ((CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END)/(1+(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END))) END as match_probability,\n    \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_match_weight_parts\n    \n    ) \n    select 0 as comparison_vector_value,\n           avg(match_probability) as m_count,\n           avg(1-match_probability) as u_count,\n           '_probability_two_random_records_match' as output_column_name\n    from __splink__df_predict\n    )\n        ...\n                             ^",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSplinkException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 45\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m linker\u001b[39m.\u001b[39;49mestimate_parameters_using_expectation_maximisation(\u001b[39m\"\u001b[39;49m\u001b[39ml.date_of_birth = r.date_of_birth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:1372\u001b[0m, in \u001b[0;36mLinker.estimate_parameters_using_expectation_maximisation\u001b[1;34m(self, blocking_rule, comparisons_to_deactivate, comparison_levels_to_reverse_blocking_rule, fix_probability_two_random_records_match, fix_m_probabilities, fix_u_probabilities, populate_probability_two_random_records_match_from_trained_values)\u001b[0m\n\u001b[0;32m   1351\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1352\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mWARNING: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1353\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou have provided comparisons_to_deactivate but not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mas an exact match.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1360\u001b[0m         )\n\u001b[0;32m   1362\u001b[0m em_training_session \u001b[39m=\u001b[39m EMTrainingSession(\n\u001b[0;32m   1363\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1364\u001b[0m     blocking_rule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     comparison_levels_to_reverse_blocking_rule\u001b[39m=\u001b[39mcomparison_levels_to_reverse_blocking_rule,  \u001b[39m# noqa 501\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m )\n\u001b[1;32m-> 1372\u001b[0m em_training_session\u001b[39m.\u001b[39;49m_train()\n\u001b[0;32m   1374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_populate_m_u_from_trained_values()\n\u001b[0;32m   1376\u001b[0m \u001b[39mif\u001b[39;00m populate_probability_two_random_records_match_from_trained_values:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\em_training_session.py:191\u001b[0m, in \u001b[0;36mEMTrainingSession._train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[39mraise\u001b[39;00m EMTrainingException(\n\u001b[0;32m    176\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining rule \u001b[39m\u001b[39m{\u001b[39;00mbr_sql\u001b[39m}\u001b[39;00m\u001b[39m resulted in no record pairs.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis means that in the supplied data set \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe number of comparisons that will be generated by a blocking rule.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    188\u001b[0m \u001b[39m# Compute the new params, populating the paramters in the copied settings object\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39m# At this stage, we do not overwrite any of the parameters\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39m# in the original (main) setting object\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m expectation_maximisation(\u001b[39mself\u001b[39;49m, cvv)\n\u001b[0;32m    193\u001b[0m rule \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking_rule_for_training\u001b[39m.\u001b[39mblocking_rule\n\u001b[0;32m    194\u001b[0m training_desc \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEM, blocked on: \u001b[39m\u001b[39m{\u001b[39;00mrule\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\expectation_maximisation.py:172\u001b[0m, in \u001b[0;36mexpectation_maximisation\u001b[1;34m(em_training_session, df_comparison_vector_values)\u001b[0m\n\u001b[0;32m    170\u001b[0m sql \u001b[39m=\u001b[39m compute_new_parameters_sql(settings_obj)\n\u001b[0;32m    171\u001b[0m linker\u001b[39m.\u001b[39m_enqueue_sql(sql, \u001b[39m\"\u001b[39m\u001b[39m__splink__m_u_counts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m df_params \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39;49m_execute_sql_pipeline([df_comparison_vector_values])\n\u001b[0;32m    173\u001b[0m param_records \u001b[39m=\u001b[39m df_params\u001b[39m.\u001b[39mas_pandas_dataframe()\n\u001b[0;32m    174\u001b[0m param_records \u001b[39m=\u001b[39m compute_proportions_for_new_parameters(param_records)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:523\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[1;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    516\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sql_to_splink_dataframe_checking_cache(\n\u001b[0;32m    517\u001b[0m         sql_gen,\n\u001b[0;32m    518\u001b[0m         output_tablename_templated,\n\u001b[0;32m    519\u001b[0m         materialise_as_hash,\n\u001b[0;32m    520\u001b[0m         use_cache,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 523\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    524\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:516\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[1;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    513\u001b[0m output_tablename_templated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mqueue[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput_table_name\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sql_to_splink_dataframe_checking_cache(\n\u001b[0;32m    517\u001b[0m         sql_gen,\n\u001b[0;32m    518\u001b[0m         output_tablename_templated,\n\u001b[0;32m    519\u001b[0m         materialise_as_hash,\n\u001b[0;32m    520\u001b[0m         use_cache,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    523\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:695\u001b[0m, in \u001b[0;36mLinker._sql_to_splink_dataframe_checking_cache\u001b[1;34m(self, sql, output_tablename_templated, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mprint\u001b[39m(sql)\n\u001b[0;32m    694\u001b[0m \u001b[39mif\u001b[39;00m materialise_as_hash:\n\u001b[1;32m--> 695\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_against_backend(\n\u001b[0;32m    696\u001b[0m         sql, output_tablename_templated, table_name_hash\n\u001b[0;32m    697\u001b[0m     )\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_sql_against_backend(\n\u001b[0;32m    700\u001b[0m         sql,\n\u001b[0;32m    701\u001b[0m         output_tablename_templated,\n\u001b[0;32m    702\u001b[0m         output_tablename_templated,\n\u001b[0;32m    703\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\duckdb\\duckdb_linker.py:176\u001b[0m, in \u001b[0;36mDuckDBLinker._execute_sql_against_backend\u001b[1;34m(self, sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_delete_table_from_database(physical_name)\n\u001b[0;32m    171\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    172\u001b[0m \u001b[39mCREATE TABLE \u001b[39m\u001b[39m{\u001b[39;00mphysical_name\u001b[39m}\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39mAS\u001b[39m\n\u001b[0;32m    174\u001b[0m \u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00msql\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m--> 176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_and_run_sql_execution(sql, templated_name, physical_name)\n\u001b[0;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m DuckDBLinkerDataFrame(templated_name, physical_name, \u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:583\u001b[0m, in \u001b[0;36mLinker._log_and_run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sql_execution(final_sql, templated_name, physical_name)\n\u001b[0;32m    582\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mraise\u001b[39;00m SplinkException(\n\u001b[0;32m    584\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError executing the following sql for table \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00mtemplated_name\u001b[39m}\u001b[39;00m\u001b[39m`(\u001b[39m\u001b[39m{\u001b[39;00mphysical_name\u001b[39m}\u001b[39;00m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfinal_sql\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mSplinkException\u001b[0m: Error executing the following sql for table `__splink__m_u_counts`(__splink__m_u_counts_0aaeffeca):\n\n        CREATE TABLE __splink__m_u_counts_0aaeffeca\n        AS\n        (WITH __splink__df_comparison_vectors as (select * from __splink__df_comparison_vectors_9a9886873), \n__splink__df_match_weight_parts as (\n    select \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_comparison_vectors\n    ), \n__splink__df_predict as (\n    select\n    log2(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END) as match_weight,\n    CASE WHEN  THEN 1.0 ELSE ((CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END)/(1+(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END))) END as match_probability,\n    \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_match_weight_parts\n    \n    ) \n    select 0 as comparison_vector_value,\n           avg(match_probability) as m_count,\n           avg(1-match_probability) as u_count,\n           '_probability_two_random_records_match' as output_column_name\n    from __splink__df_predict\n    )\n        "
     ]
    }
   ],
   "source": [
    "linker.estimate_parameters_using_expectation_maximisation(\"l.date_of_birth = r.date_of_birth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SplinkException",
     "evalue": "Error executing the following sql for table `__splink__df_predict`(__splink__df_predict_6b3b73946):\n\n        CREATE TABLE __splink__df_predict_6b3b73946\n        AS\n        (WITH __splink__df_concat_with_tf as (select * from __splink__df_concat_with_tf_335fab517), \n__splink__df_concat_with_tf_left as (\n        select * from __splink__df_concat_with_tf\n        where source_dataset = '__splink__input_table_0'\n        ), \n__splink_df_concat_with_tf_right as (\n        select * from __splink__df_concat_with_tf\n        where source_dataset = '__splink__input_table_1'\n        ), \n__splink__df_blocked as (\n            select\n            \"l\".\"source_dataset\" as \"source_dataset_l\", \"r\".\"source_dataset\" as \"source_dataset_r\", \"l\".\"rec_id\" as \"rec_id_l\", \"r\".\"rec_id\" as \"rec_id_r\"\n            , '0' as match_key\n            from __splink__df_concat_with_tf_left as l\n            inner join __splink_df_concat_with_tf_right as r\n            on\n            1=1\n            \n             where 1=1 \n            ), \n__splink__df_comparison_vectors as (\n    select \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_blocked\n    ), \n__splink__df_match_weight_parts as (\n    select \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_comparison_vectors\n    ) \n    select\n    log2(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END) as match_weight,\n    CASE WHEN  THEN 1.0 ELSE ((CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END)/(1+(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END))) END as match_probability,\n    \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_match_weight_parts\n    \n    )\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:581\u001b[0m, in \u001b[0;36mLinker._log_and_run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sql_execution(final_sql, templated_name, physical_name)\n\u001b[0;32m    582\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\duckdb\\duckdb_linker.py:181\u001b[0m, in \u001b[0;36mDuckDBLinker._run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_sql_execution\u001b[39m(\u001b[39mself\u001b[39m, final_sql, templated_name, physical_name):\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_con\u001b[39m.\u001b[39;49mexecute(final_sql)\n",
      "\u001b[1;31mParserException\u001b[0m: Parser Error: syntax error at or near \"THEN\"\nLINE 33:     log2(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END) as match_weight,\n    CASE WHEN  THEN 1.0 ELSE ((CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END)/(1+(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END))) END as match_probability,\n    \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_match_weight_parts\n    \n    )\n        ...\n                             ^",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSplinkException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 46\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pairwise_predictions \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39;49mpredict()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m clusters \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39mcluster_pairwise_predictions_at_threshold(pairwise_predictions, \u001b[39m0.70\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m resulted_dataframe \u001b[39m=\u001b[39m clusters\u001b[39m.\u001b[39mas_pandas_dataframe()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:1459\u001b[0m, in \u001b[0;36mLinker.predict\u001b[1;34m(self, threshold_match_probability, threshold_match_weight, materialise_after_computing_term_frequencies)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39mfor\u001b[39;00m sql \u001b[39min\u001b[39;00m sqls:\n\u001b[0;32m   1457\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enqueue_sql(sql[\u001b[39m\"\u001b[39m\u001b[39msql\u001b[39m\u001b[39m\"\u001b[39m], sql[\u001b[39m\"\u001b[39m\u001b[39moutput_table_name\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1459\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_pipeline(input_dataframes)\n\u001b[0;32m   1460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_warning()\n\u001b[0;32m   1461\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:523\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[1;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    516\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sql_to_splink_dataframe_checking_cache(\n\u001b[0;32m    517\u001b[0m         sql_gen,\n\u001b[0;32m    518\u001b[0m         output_tablename_templated,\n\u001b[0;32m    519\u001b[0m         materialise_as_hash,\n\u001b[0;32m    520\u001b[0m         use_cache,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 523\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    524\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:516\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[1;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    513\u001b[0m output_tablename_templated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mqueue[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput_table_name\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sql_to_splink_dataframe_checking_cache(\n\u001b[0;32m    517\u001b[0m         sql_gen,\n\u001b[0;32m    518\u001b[0m         output_tablename_templated,\n\u001b[0;32m    519\u001b[0m         materialise_as_hash,\n\u001b[0;32m    520\u001b[0m         use_cache,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    523\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:695\u001b[0m, in \u001b[0;36mLinker._sql_to_splink_dataframe_checking_cache\u001b[1;34m(self, sql, output_tablename_templated, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mprint\u001b[39m(sql)\n\u001b[0;32m    694\u001b[0m \u001b[39mif\u001b[39;00m materialise_as_hash:\n\u001b[1;32m--> 695\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_against_backend(\n\u001b[0;32m    696\u001b[0m         sql, output_tablename_templated, table_name_hash\n\u001b[0;32m    697\u001b[0m     )\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_sql_against_backend(\n\u001b[0;32m    700\u001b[0m         sql,\n\u001b[0;32m    701\u001b[0m         output_tablename_templated,\n\u001b[0;32m    702\u001b[0m         output_tablename_templated,\n\u001b[0;32m    703\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\duckdb\\duckdb_linker.py:176\u001b[0m, in \u001b[0;36mDuckDBLinker._execute_sql_against_backend\u001b[1;34m(self, sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_delete_table_from_database(physical_name)\n\u001b[0;32m    171\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    172\u001b[0m \u001b[39mCREATE TABLE \u001b[39m\u001b[39m{\u001b[39;00mphysical_name\u001b[39m}\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39mAS\u001b[39m\n\u001b[0;32m    174\u001b[0m \u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00msql\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m--> 176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_and_run_sql_execution(sql, templated_name, physical_name)\n\u001b[0;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m DuckDBLinkerDataFrame(templated_name, physical_name, \u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:583\u001b[0m, in \u001b[0;36mLinker._log_and_run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sql_execution(final_sql, templated_name, physical_name)\n\u001b[0;32m    582\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mraise\u001b[39;00m SplinkException(\n\u001b[0;32m    584\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError executing the following sql for table \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00mtemplated_name\u001b[39m}\u001b[39;00m\u001b[39m`(\u001b[39m\u001b[39m{\u001b[39;00mphysical_name\u001b[39m}\u001b[39;00m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfinal_sql\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mSplinkException\u001b[0m: Error executing the following sql for table `__splink__df_predict`(__splink__df_predict_6b3b73946):\n\n        CREATE TABLE __splink__df_predict_6b3b73946\n        AS\n        (WITH __splink__df_concat_with_tf as (select * from __splink__df_concat_with_tf_335fab517), \n__splink__df_concat_with_tf_left as (\n        select * from __splink__df_concat_with_tf\n        where source_dataset = '__splink__input_table_0'\n        ), \n__splink_df_concat_with_tf_right as (\n        select * from __splink__df_concat_with_tf\n        where source_dataset = '__splink__input_table_1'\n        ), \n__splink__df_blocked as (\n            select\n            \"l\".\"source_dataset\" as \"source_dataset_l\", \"r\".\"source_dataset\" as \"source_dataset_r\", \"l\".\"rec_id\" as \"rec_id_l\", \"r\".\"rec_id\" as \"rec_id_r\"\n            , '0' as match_key\n            from __splink__df_concat_with_tf_left as l\n            inner join __splink_df_concat_with_tf_right as r\n            on\n            1=1\n            \n             where 1=1 \n            ), \n__splink__df_comparison_vectors as (\n    select \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_blocked\n    ), \n__splink__df_match_weight_parts as (\n    select \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_comparison_vectors\n    ) \n    select\n    log2(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END) as match_weight,\n    CASE WHEN  THEN 1.0 ELSE ((CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END)/(1+(CASE WHEN  THEN cast('infinity' as double) ELSE cast(0.00010001000100010001 as double) *  END))) END as match_probability,\n    \"source_dataset_l\",\"source_dataset_r\",\"rec_id_l\",\"rec_id_r\" \n    from __splink__df_match_weight_parts\n    \n    )\n        "
     ]
    }
   ],
   "source": [
    "pairwise_predictions = linker.predict()\n",
    "clusters = linker.cluster_pairwise_predictions_at_threshold(pairwise_predictions, 0.70)\n",
    "resulted_dataframe = clusters.as_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(r'../../Dataset/Input Datasets/EHR_Linkage(1).xlsx')\n",
    "# df2 = pd.read_excel(r'../../Dataset/Input Datasets/EHR_Linkage(2).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns\n",
    "df1 = df1[['given_name', 'surname', 'birthdate',\n",
    "       'address', 'county', 'city', 'state', 'zip', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[:6000]\n",
    "df1 = df1[:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_excel(r'../../Dataset/Input Datasets/EHR_Linkage(1).xlsx',index = False)\n",
    "df2.to_excel(r'../../Dataset/Input Datasets/EHR_Linkage(2).xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GauravS15\\AppData\\Local\\Temp\\ipykernel_18120\\2009753982.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(df1[:6000])\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.append(df1[:6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_excel(r'../../Dataset/Input Datasets/EHR_Deduplication.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GauravS15\\AppData\\Local\\Temp\\ipykernel_25268\\2091786775.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['state'][1] = 'parag'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'parag'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['state'][1] = 'parag'\n",
    "df['state'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: DeprecationWarning: invalid escape sequence \\I\n",
      "<>:1: DeprecationWarning: invalid escape sequence \\I\n",
      "C:\\Users\\GauravS15\\AppData\\Local\\Temp\\ipykernel_25268\\444884831.py:1: DeprecationWarning: invalid escape sequence \\I\n",
      "  df = pd.read_excel('../../Dataset\\Input Datasets\\EHR_Deduplication.xlsx')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('../../Dataset\\Input Datasets\\EHR_Deduplication.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SplinkException",
     "evalue": "Error executing the following sql for table `__splink__df_all_column_value_frequencies`(__splink__df_all_column_value_frequencies_3a851632c):\n\n        CREATE TABLE __splink__df_all_column_value_frequencies_3a851632c\n        AS\n        (WITH __splink__df_concat as (select * from __splink__df_concat_with_tf_c53fb6107) \n        select * from\n        (select\n            count(*) as value_count,\n            'given_name' as group_name,\n            cast(given_name as varchar) as value,\n            (select count(given_name) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct given_name) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where given_name is not null\n        group by given_name\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'surname' as group_name,\n            cast(surname as varchar) as value,\n            (select count(surname) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct surname) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where surname is not null\n        group by surname\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'birthdate' as group_name,\n            cast(birthdate as varchar) as value,\n            (select count(birthdate) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct birthdate) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where birthdate is not null\n        group by birthdate\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'address' as group_name,\n            cast(address as varchar) as value,\n            (select count(address) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct address) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where address is not null\n        group by address\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'county' as group_name,\n            cast(county as varchar) as value,\n            (select count(county) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct county) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where county is not null\n        group by county\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'city' as group_name,\n            cast(city as varchar) as value,\n            (select count(city) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct city) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where city is not null\n        group by city\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'state' as group_name,\n            cast(state as varchar) as value,\n            (select count(state) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct state) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where state is not null\n        group by state\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'zip' as group_name,\n            cast(zip as varchar) as value,\n            (select count(zip) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct zip) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where zip is not null\n        group by zip\n        order by count(*) desc)\n        )\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBinderException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:581\u001b[0m, in \u001b[0;36mLinker._log_and_run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sql_execution(final_sql, templated_name, physical_name)\n\u001b[0;32m    582\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\duckdb\\duckdb_linker.py:181\u001b[0m, in \u001b[0;36mDuckDBLinker._run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_sql_execution\u001b[39m(\u001b[39mself\u001b[39m, final_sql, templated_name, physical_name):\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_con\u001b[39m.\u001b[39;49mexecute(final_sql)\n",
      "\u001b[1;31mBinderException\u001b[0m: Binder Error: Referenced column \"birthdate\" not found in FROM clause!\nCandidate bindings: \"__splink__df_concat.state\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSplinkException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 58\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sample_dataframe \u001b[39m=\u001b[39m df[[\u001b[39m'\u001b[39m\u001b[39mgiven_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msurname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbirthdate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maddress\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcounty\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mzip\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m spec_missing \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39mmissingness_chart()\u001b[39m.\u001b[39mspec\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m spec_columns \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39;49mprofile_columns([\u001b[39m'\u001b[39;49m\u001b[39mgiven_name\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msurname\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mbirthdate\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39maddress\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcounty\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcity\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mstate\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mspec\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:1782\u001b[0m, in \u001b[0;36mLinker.profile_columns\u001b[1;34m(self, column_expressions, top_n, bottom_n)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprofile_columns\u001b[39m(\n\u001b[0;32m   1780\u001b[0m     \u001b[39mself\u001b[39m, column_expressions: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m], top_n\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, bottom_n\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[1;32m-> 1782\u001b[0m     \u001b[39mreturn\u001b[39;00m profile_columns(\u001b[39mself\u001b[39;49m, column_expressions, top_n\u001b[39m=\u001b[39;49mtop_n, bottom_n\u001b[39m=\u001b[39;49mbottom_n)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\profile_data.py:208\u001b[0m, in \u001b[0;36mprofile_columns\u001b[1;34m(linker, column_expressions, top_n, bottom_n)\u001b[0m\n\u001b[0;32m    203\u001b[0m sql \u001b[39m=\u001b[39m _col_or_expr_frequencies_raw_data_sql(\n\u001b[0;32m    204\u001b[0m     column_expressions_raw, \u001b[39m\"\u001b[39m\u001b[39m__splink__df_concat\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m )\n\u001b[0;32m    207\u001b[0m linker\u001b[39m.\u001b[39m_enqueue_sql(sql, \u001b[39m\"\u001b[39m\u001b[39m__splink__df_all_column_value_frequencies\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 208\u001b[0m df_raw \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39;49m_execute_sql_pipeline(input_dataframes, materialise_as_hash\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    210\u001b[0m sqls \u001b[39m=\u001b[39m _get_df_percentiles()\n\u001b[0;32m    211\u001b[0m \u001b[39mfor\u001b[39;00m sql \u001b[39min\u001b[39;00m sqls:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:523\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[1;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    516\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sql_to_splink_dataframe_checking_cache(\n\u001b[0;32m    517\u001b[0m         sql_gen,\n\u001b[0;32m    518\u001b[0m         output_tablename_templated,\n\u001b[0;32m    519\u001b[0m         materialise_as_hash,\n\u001b[0;32m    520\u001b[0m         use_cache,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 523\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    524\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:516\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[1;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    513\u001b[0m output_tablename_templated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mqueue[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput_table_name\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sql_to_splink_dataframe_checking_cache(\n\u001b[0;32m    517\u001b[0m         sql_gen,\n\u001b[0;32m    518\u001b[0m         output_tablename_templated,\n\u001b[0;32m    519\u001b[0m         materialise_as_hash,\n\u001b[0;32m    520\u001b[0m         use_cache,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    523\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:695\u001b[0m, in \u001b[0;36mLinker._sql_to_splink_dataframe_checking_cache\u001b[1;34m(self, sql, output_tablename_templated, materialise_as_hash, use_cache)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mprint\u001b[39m(sql)\n\u001b[0;32m    694\u001b[0m \u001b[39mif\u001b[39;00m materialise_as_hash:\n\u001b[1;32m--> 695\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_against_backend(\n\u001b[0;32m    696\u001b[0m         sql, output_tablename_templated, table_name_hash\n\u001b[0;32m    697\u001b[0m     )\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_sql_against_backend(\n\u001b[0;32m    700\u001b[0m         sql,\n\u001b[0;32m    701\u001b[0m         output_tablename_templated,\n\u001b[0;32m    702\u001b[0m         output_tablename_templated,\n\u001b[0;32m    703\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\duckdb\\duckdb_linker.py:176\u001b[0m, in \u001b[0;36mDuckDBLinker._execute_sql_against_backend\u001b[1;34m(self, sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_delete_table_from_database(physical_name)\n\u001b[0;32m    171\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    172\u001b[0m \u001b[39mCREATE TABLE \u001b[39m\u001b[39m{\u001b[39;00mphysical_name\u001b[39m}\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39mAS\u001b[39m\n\u001b[0;32m    174\u001b[0m \u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00msql\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m--> 176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_and_run_sql_execution(sql, templated_name, physical_name)\n\u001b[0;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m DuckDBLinkerDataFrame(templated_name, physical_name, \u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\splink\\linker.py:583\u001b[0m, in \u001b[0;36mLinker._log_and_run_sql_execution\u001b[1;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sql_execution(final_sql, templated_name, physical_name)\n\u001b[0;32m    582\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mraise\u001b[39;00m SplinkException(\n\u001b[0;32m    584\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError executing the following sql for table \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00mtemplated_name\u001b[39m}\u001b[39;00m\u001b[39m`(\u001b[39m\u001b[39m{\u001b[39;00mphysical_name\u001b[39m}\u001b[39;00m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfinal_sql\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mSplinkException\u001b[0m: Error executing the following sql for table `__splink__df_all_column_value_frequencies`(__splink__df_all_column_value_frequencies_3a851632c):\n\n        CREATE TABLE __splink__df_all_column_value_frequencies_3a851632c\n        AS\n        (WITH __splink__df_concat as (select * from __splink__df_concat_with_tf_c53fb6107) \n        select * from\n        (select\n            count(*) as value_count,\n            'given_name' as group_name,\n            cast(given_name as varchar) as value,\n            (select count(given_name) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct given_name) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where given_name is not null\n        group by given_name\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'surname' as group_name,\n            cast(surname as varchar) as value,\n            (select count(surname) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct surname) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where surname is not null\n        group by surname\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'birthdate' as group_name,\n            cast(birthdate as varchar) as value,\n            (select count(birthdate) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct birthdate) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where birthdate is not null\n        group by birthdate\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'address' as group_name,\n            cast(address as varchar) as value,\n            (select count(address) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct address) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where address is not null\n        group by address\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'county' as group_name,\n            cast(county as varchar) as value,\n            (select count(county) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct county) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where county is not null\n        group by county\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'city' as group_name,\n            cast(city as varchar) as value,\n            (select count(city) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct city) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where city is not null\n        group by city\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'state' as group_name,\n            cast(state as varchar) as value,\n            (select count(state) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct state) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where state is not null\n        group by state\n        order by count(*) desc)\n         union all \n        select * from\n        (select\n            count(*) as value_count,\n            'zip' as group_name,\n            cast(zip as varchar) as value,\n            (select count(zip) from __splink__df_concat) as total_non_null_rows,\n            (select count(*) from __splink__df_concat) as total_rows_inc_nulls,\n            (select count(distinct zip) from __splink__df_concat)\n                as distinct_value_count\n        from __splink__df_concat\n        where zip is not null\n        group by zip\n        order by count(*) desc)\n        )\n        "
     ]
    }
   ],
   "source": [
    "dataframe_columns = ['given_name', 'surname', 'birthdate', 'address', 'county', 'city', 'state', 'zip']\n",
    "sample_dataframe = df[['given_name', 'surname', 'birthdate', 'address', 'county', 'city', 'state', 'zip']]\n",
    "spec_missing = linker.missingness_chart().spec\n",
    "spec_columns = linker.profile_columns(['given_name', 'surname', 'birthdate', 'address', 'county', 'city', 'state', 'zip']).spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplicate Table Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('deduplicate.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe_ehr = pd.read_excel('..\\..\\Dataset\\Input Datasets\\Febrl3_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rec_id', 'given_name', 'surname', 'street_number', 'address_1',\n",
      "       'address_2', 'suburb', 'postcode', 'state', 'date_of_birth',\n",
      "       'soc_sec_id', 'cluster'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_ehr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_ehr = dataframe_ehr.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"Create table if not Exists {'Febrl3_Data'} ('rec_id' text, 'given_name' text, 'surname' text, 'street_number' integer, 'address_1' text, 'address_2' text, 'suburb' text, 'postcode' real, 'state' text, 'date_of_birth' real, 'soc_sec_id' real, 'cluster' integer)\"\n",
    "c.execute(query)\n",
    "dataframe_ehr.to_sql('Febrl3_Data',conn,if_exists = 'replace',index = False)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"Create table if not Exists {'EHR_Deduplication'} ('given_name' text, 'surname' text, 'birthdate' real, 'address' text, 'county' text, 'city' text, 'state' text, 'zip' real, 'latitude' real, 'longitude' real)\"\n",
    "c.execute(query)\n",
    "dataframe_ehr.to_sql('EHR_Deduplication',conn,if_exists = 'replace',index = False)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1ce49b52ab0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c.execute(\"Drop table Febrl_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rec_id', 'given_name', 'surname', 'street_number', 'address_1',\n",
      "       'address_2', 'suburb', 'postcode', 'state', 'date_of_birth',\n",
      "       'soc_sec_id', 'cluster'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "r_df = pd.read_sql( \"select * from Febrl3_Data\", conn)\n",
    "print(r_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Linkage Table Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('data_linkage.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe_ehr = pd.read_excel('..\\..\\Dataset\\Input Datasets\\Febrl_Data4B.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rec_id', 'given_name', 'surname', 'street_number', 'address_1',\n",
      "       'address_2', 'suburb', 'postcode', 'state', 'date_of_birth',\n",
      "       'soc_sec_id', 'cluster'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_ehr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_ehr = dataframe_ehr.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"Create table if not Exists {'Febrl_Data4B'} ('rec_id' text, 'given_name' text, 'surname' text, 'street_number' integer, 'address_1' text, 'address_2' text, 'suburb' text, 'postcode' real, 'state' text, 'date_of_birth' real, 'soc_sec_id' real, 'cluster' integer)\"\n",
    "c.execute(query)\n",
    "dataframe_ehr.to_sql('Febrl_Data4B',conn,if_exists = 'replace',index = False)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe_ehr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 91\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y154sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCreate table if not Exists \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mEHR_Deduplication\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgiven_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m text, \u001b[39m\u001b[39m'\u001b[39m\u001b[39msurname\u001b[39m\u001b[39m'\u001b[39m\u001b[39m text, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbirthdate\u001b[39m\u001b[39m'\u001b[39m\u001b[39m real, \u001b[39m\u001b[39m'\u001b[39m\u001b[39maddress\u001b[39m\u001b[39m'\u001b[39m\u001b[39m text, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcounty\u001b[39m\u001b[39m'\u001b[39m\u001b[39m text, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcity\u001b[39m\u001b[39m'\u001b[39m\u001b[39m text, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m\u001b[39m text, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mzip\u001b[39m\u001b[39m'\u001b[39m\u001b[39m real, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatitude\u001b[39m\u001b[39m'\u001b[39m\u001b[39m real, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongitude\u001b[39m\u001b[39m'\u001b[39m\u001b[39m real)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y154sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m c\u001b[39m.\u001b[39mexecute(query)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y154sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataframe_ehr\u001b[39m.\u001b[39mto_sql(\u001b[39m'\u001b[39m\u001b[39mEHR_Deduplication\u001b[39m\u001b[39m'\u001b[39m,conn,if_exists \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m'\u001b[39m,index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y154sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m conn\u001b[39m.\u001b[39mcommit()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataframe_ehr' is not defined"
     ]
    }
   ],
   "source": [
    "query = f\"Create table if not Exists {'EHR_Deduplication'} ('given_name' text, 'surname' text, 'birthdate' real, 'address' text, 'county' text, 'city' text, 'state' text, 'zip' real, 'latitude' real, 'longitude' real)\"\n",
    "c.execute(query)\n",
    "dataframe_ehr.to_sql('EHR_Deduplication',conn,if_exists = 'replace',index = False)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(\"Drop table EHR_Deduplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rec_id', 'given_name', 'surname', 'street_number', 'address_1',\n",
      "       'address_2', 'suburb', 'postcode', 'state', 'date_of_birth',\n",
      "       'soc_sec_id', 'cluster'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "r_df = pd.read_sql(\"select * from Febrl_Data4B\",conn)\n",
    "print(r_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonylink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clkhash\n",
    "from clkhash import clk\n",
    "from clkhash.field_formats import *\n",
    "from clkhash.schema import Schema\n",
    "from clkhash.comparators import NgramComparison\n",
    "from clkhash.serialization import serialize_bitarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage import datasets\n",
    "dfA,dfB = datasets.load_febrl4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_csv = io.StringIO()\n",
    "dfA.to_csv(a_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    Ignore('rec_id'),\n",
    "    StringSpec('given_name', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    StringSpec('surname', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    IntegerSpec('street_number', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(300), missing_value=MissingValueSpec(sentinel=''))),\n",
    "    StringSpec('address_1', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    StringSpec('address_2', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    StringSpec('suburb', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    IntegerSpec('postcode', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(300))),\n",
    "    StringSpec('state', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    IntegerSpec('date_of_birth', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(300), missing_value=MissingValueSpec(sentinel=''))),\n",
    "    Ignore('soc_sec_id')\n",
    "]\n",
    "\n",
    "schema = Schema(fields, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating CLKs: 100%|█████████▉| 5.00k/5.00k [00:02<00:00, 2.24kclk/s, mean=944, std=14.4]\n"
     ]
    }
   ],
   "source": [
    "secret = 'secret'\n",
    "a_csv.seek(0)\n",
    "hashed_data_a = clk.generate_clk_from_csv(a_csv, secret, schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('..\\..\\Dataset\\Input Datasets\\Febrl1_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.given_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec_id             0\n",
       "given_name        44\n",
       "surname           18\n",
       "street_number     45\n",
       "address_1         25\n",
       "address_2        115\n",
       "suburb            18\n",
       "postcode           0\n",
       "state             15\n",
       "date_of_birth     41\n",
       "soc_sec_id         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_dataframe = dataframe.columns\n",
    "for col in columns_of_dataframe:\n",
    "    dataframe[col] = dataframe[col].apply(lambda x: x if str(x) == 'nan' else str(x).replace(str(x)[-3::],\"XXX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>soc_sec_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rec-223-XXX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>walXXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>tullaroop strXXX</td>\n",
       "      <td>willaXXX</td>\n",
       "      <td>st jaXXX</td>\n",
       "      <td>4XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1908120XXX</td>\n",
       "      <td>6988XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rec-122-XXX</td>\n",
       "      <td>lachXXX</td>\n",
       "      <td>beXXX</td>\n",
       "      <td>6XXX</td>\n",
       "      <td>giblin strXXX</td>\n",
       "      <td>killarXXX</td>\n",
       "      <td>bittXXX</td>\n",
       "      <td>4XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1999021XXX</td>\n",
       "      <td>7364XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rec-373-XXX</td>\n",
       "      <td>deaXXX</td>\n",
       "      <td>sondergXXX</td>\n",
       "      <td>4XXX</td>\n",
       "      <td>goldfinch circXXX</td>\n",
       "      <td>koolXXX</td>\n",
       "      <td>canterbXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1960021XXX</td>\n",
       "      <td>2635XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rec-10-duXXX</td>\n",
       "      <td>kaXXX</td>\n",
       "      <td>harringXXX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maltby circXXX</td>\n",
       "      <td>coalXXX</td>\n",
       "      <td>coolaXXX</td>\n",
       "      <td>3XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1915061XXX</td>\n",
       "      <td>9004XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rec-227-XXX</td>\n",
       "      <td>lXXX</td>\n",
       "      <td>purXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>ramsay plXXX</td>\n",
       "      <td>mirXXX</td>\n",
       "      <td>garbXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1983102XXX</td>\n",
       "      <td>8099XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>rec-188-duXXX</td>\n",
       "      <td>stephaXXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>bainton crescXXX</td>\n",
       "      <td>masonic memorial villXXX</td>\n",
       "      <td>maryboroXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1942100XXX</td>\n",
       "      <td>3997XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>rec-334-duXXX</td>\n",
       "      <td>nichoXXX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28XXX</td>\n",
       "      <td>britten-jonues drXXX</td>\n",
       "      <td>jabaru coXXX</td>\n",
       "      <td>paddingXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1997042XXX</td>\n",
       "      <td>5062XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>rec-469-duXXX</td>\n",
       "      <td>lachXXX</td>\n",
       "      <td>katsiaXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>paul coe cdrescXXX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>casXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1938040XXX</td>\n",
       "      <td>4112XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>rec-350-duXXX</td>\n",
       "      <td>moniXXX</td>\n",
       "      <td>gergXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>harwoos coXXX</td>\n",
       "      <td>hyberni a pXXX</td>\n",
       "      <td>sherwXXX</td>\n",
       "      <td>2XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1979080XXX</td>\n",
       "      <td>7375XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>rec-212-XXX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mcveXXX</td>\n",
       "      <td>4XXX</td>\n",
       "      <td>bougainville strXXX</td>\n",
       "      <td>kimberXXX</td>\n",
       "      <td>ourimXXX</td>\n",
       "      <td>6XXX</td>\n",
       "      <td>XXX</td>\n",
       "      <td>1936021XXX</td>\n",
       "      <td>8243XXX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            rec_id given_name     surname street_number             address_1  \\\n",
       "0      rec-223-XXX        NaN      walXXX           XXX      tullaroop strXXX   \n",
       "1      rec-122-XXX    lachXXX       beXXX          6XXX         giblin strXXX   \n",
       "2      rec-373-XXX     deaXXX  sondergXXX          4XXX     goldfinch circXXX   \n",
       "3     rec-10-duXXX      kaXXX  harringXXX           NaN        maltby circXXX   \n",
       "4      rec-227-XXX       lXXX      purXXX          2XXX          ramsay plXXX   \n",
       "..             ...        ...         ...           ...                   ...   \n",
       "995  rec-188-duXXX  stephaXXX         XXX          2XXX      bainton crescXXX   \n",
       "996  rec-334-duXXX   nichoXXX         NaN         28XXX  britten-jonues drXXX   \n",
       "997  rec-469-duXXX    lachXXX   katsiaXXX          2XXX    paul coe cdrescXXX   \n",
       "998  rec-350-duXXX    moniXXX     gergXXX          2XXX         harwoos coXXX   \n",
       "999    rec-212-XXX        NaN     mcveXXX          4XXX   bougainville strXXX   \n",
       "\n",
       "                    address_2       suburb postcode state date_of_birth  \\\n",
       "0                    willaXXX     st jaXXX     4XXX   XXX    1908120XXX   \n",
       "1                   killarXXX      bittXXX     4XXX   XXX    1999021XXX   \n",
       "2                     koolXXX   canterbXXX     2XXX   XXX    1960021XXX   \n",
       "3                     coalXXX     coolaXXX     3XXX   XXX    1915061XXX   \n",
       "4                      mirXXX      garbXXX     2XXX   XXX    1983102XXX   \n",
       "..                        ...          ...      ...   ...           ...   \n",
       "995  masonic memorial villXXX  maryboroXXX     2XXX   XXX    1942100XXX   \n",
       "996              jabaru coXXX   paddingXXX     2XXX   XXX    1997042XXX   \n",
       "997                       NaN       casXXX     2XXX   XXX    1938040XXX   \n",
       "998            hyberni a pXXX     sherwXXX     2XXX   XXX    1979080XXX   \n",
       "999                 kimberXXX     ourimXXX     6XXX   XXX    1936021XXX   \n",
       "\n",
       "    soc_sec_id  \n",
       "0      6988XXX  \n",
       "1      7364XXX  \n",
       "2      2635XXX  \n",
       "3      9004XXX  \n",
       "4      8099XXX  \n",
       "..         ...  \n",
       "995    3997XXX  \n",
       "996    5062XXX  \n",
       "997    4112XXX  \n",
       "998    7375XXX  \n",
       "999    8243XXX  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec_id             0\n",
       "given_name        44\n",
       "surname           18\n",
       "street_number     45\n",
       "address_1         25\n",
       "address_2        115\n",
       "suburb            18\n",
       "postcode           0\n",
       "state             15\n",
       "date_of_birth     41\n",
       "soc_sec_id         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHC, Gosainganj, Sultanpur Road, अमेठी, Mohanlalganj, Lucknow, Uttar Pradesh, 227125, India\n",
      "Latitude =  26.7652312 \n",
      "\n",
      "Longitude =  81.1196719\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    " \n",
    "# calling the Nominatim tool\n",
    "loc = Nominatim(user_agent = 'myGeocode')\n",
    " \n",
    "# entering the location name\n",
    "getLoc = loc.geocode(\"Gosainganj Lucknow\")\n",
    "\n",
    "# # printing address\n",
    "print(getLoc.address)\n",
    " \n",
    "# # printing latitude and longitude\n",
    "print(\"Latitude = \", getLoc.latitude, \"\\n\")\n",
    "print(\"Longitude = \", getLoc.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correct_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 110\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y210sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y210sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataframe)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y210sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m dataframe[\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m correct_states:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y210sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         df\u001b[39m.\u001b[39mappend()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y210sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(coordinates)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'correct_states' is not defined"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent='myGeocoder')\n",
    "latitude = dataframe['latitude'].to_list()\n",
    "longitude = dataframe['longitude'].to_list()\n",
    "\n",
    "coordinates = list(zip(latitude,longitude))\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(dataframe)):\n",
    "    if dataframe['state'][i] not in correct_states:\n",
    "        df.append()\n",
    "for i in range(len(coordinates)):\n",
    "    \n",
    "    location = locator.reverse(coordinates[i])\n",
    "    \n",
    "    if 'state' in location.raw['address']:\n",
    "        dataframe['state'][i] = location.raw['address']['state'] \n",
    "\n",
    "    if 'postcode' in location.raw['address']:\n",
    "        dataframe['zip'][i] = location.raw['address']['postcode']\n",
    "    \n",
    "    if 'county' in location.raw['address']:\n",
    "        dataframe['county'][i] = location.raw['address']['county']\n",
    "    \n",
    "    if 'city' in location.raw['address']:\n",
    "        dataframe['city'][i] = location.raw['address']['city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopy\n",
    "df = pd.read_excel('..\\..\\Dataset\\Input Datasets\\EHR_Deduplication.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = dataframe['latitude'].to_list()\n",
    "longitude = dataframe['longitude'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent='myGeocoder')\n",
    "# location = geolocator.reverse((latitude,longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "GeocoderUnavailable",
     "evalue": "HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /reverse?lat=42.15196147496354&lon=-72.59895940376188&format=json&addressdetails=1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000264188881C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 918\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[0;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000264188881C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    441\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    442\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    443\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    444\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    445\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    449\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    450\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    453\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[0;32m    814\u001b[0m     )\n\u001b[1;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    816\u001b[0m         method,\n\u001b[0;32m    817\u001b[0m         url,\n\u001b[0;32m    818\u001b[0m         body,\n\u001b[0;32m    819\u001b[0m         headers,\n\u001b[0;32m    820\u001b[0m         retries,\n\u001b[0;32m    821\u001b[0m         redirect,\n\u001b[0;32m    822\u001b[0m         assert_same_host,\n\u001b[0;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[0;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[0;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[0;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[0;32m    829\u001b[0m     )\n\u001b[0;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[0;32m    814\u001b[0m     )\n\u001b[1;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    816\u001b[0m         method,\n\u001b[0;32m    817\u001b[0m         url,\n\u001b[0;32m    818\u001b[0m         body,\n\u001b[0;32m    819\u001b[0m         headers,\n\u001b[0;32m    820\u001b[0m         retries,\n\u001b[0;32m    821\u001b[0m         redirect,\n\u001b[0;32m    822\u001b[0m         assert_same_host,\n\u001b[0;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[0;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[0;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[0;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[0;32m    829\u001b[0m     )\n\u001b[0;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    789\u001b[0m )\n\u001b[0;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /reverse?lat=42.15196147496354&lon=-72.59895940376188&format=json&addressdetails=1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000264188881C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\adapters.py:457\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mget(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:542\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 542\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /reverse?lat=42.15196147496354&lon=-72.59895940376188&format=json&addressdetails=1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000264188881C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mGeocoderUnavailable\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 115\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y215sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m df\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y215sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m geolocator \u001b[39m=\u001b[39m geopy\u001b[39m.\u001b[39mNominatim(user_agent \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmyGeocode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y215sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m dataframe \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39;49mapply(get_zipcode, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, geolocator\u001b[39m=\u001b[39;49mgeolocator, lat_field\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlatitude\u001b[39;49m\u001b[39m'\u001b[39;49m, lon_field\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlongitude\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:9555\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9544\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9546\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9547\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9548\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9553\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9554\u001b[0m )\n\u001b[1;32m-> 9555\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\apply.py:139\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x):\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m func(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 115\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y215sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_zipcode\u001b[39m(df, geolocator, lat_field, lon_field):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y215sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   location \u001b[39m=\u001b[39m geolocator\u001b[39m.\u001b[39;49mreverse((df[lat_field], df[lon_field]), timeout\u001b[39m=\u001b[39;49m \u001b[39m1.5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y215sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m location\u001b[39m.\u001b[39mraw[\u001b[39m'\u001b[39m\u001b[39maddress\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m!=\u001b[39mlocation\u001b[39m.\u001b[39mraw[\u001b[39m'\u001b[39m\u001b[39maddress\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y215sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         df[\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m location\u001b[39m.\u001b[39mraw[\u001b[39m'\u001b[39m\u001b[39maddress\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m] \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\geocoders\\nominatim.py:372\u001b[0m, in \u001b[0;36mNominatim.reverse\u001b[1;34m(self, query, exactly_one, timeout, language, addressdetails, zoom, namedetails)\u001b[0m\n\u001b[0;32m    370\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.reverse: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, url)\n\u001b[0;32m    371\u001b[0m callback \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_json, exactly_one\u001b[39m=\u001b[39mexactly_one)\n\u001b[1;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_geocoder(url, callback, timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\geocoders\\base.py:368\u001b[0m, in \u001b[0;36mGeocoder._call_geocoder\u001b[1;34m(self, url, callback, timeout, is_json, headers)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[39mif\u001b[39;00m is_json:\n\u001b[1;32m--> 368\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madapter\u001b[39m.\u001b[39;49mget_json(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mreq_headers)\n\u001b[0;32m    369\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter\u001b[39m.\u001b[39mget_text(url, timeout\u001b[39m=\u001b[39mtimeout, headers\u001b[39m=\u001b[39mreq_headers)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\adapters.py:447\u001b[0m, in \u001b[0;36mRequestsAdapter.get_json\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_json\u001b[39m(\u001b[39mself\u001b[39m, url, \u001b[39m*\u001b[39m, timeout, headers):\n\u001b[1;32m--> 447\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    448\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m         \u001b[39mreturn\u001b[39;00m resp\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\adapters.py:469\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[39mraise\u001b[39;00m GeocoderServiceError(message)\n\u001b[0;32m    468\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 469\u001b[0m         \u001b[39mraise\u001b[39;00m GeocoderUnavailable(message)\n\u001b[0;32m    470\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(error, requests\u001b[39m.\u001b[39mTimeout):\n\u001b[0;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m GeocoderTimedOut(\u001b[39m\"\u001b[39m\u001b[39mService timed out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mGeocoderUnavailable\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /reverse?lat=42.15196147496354&lon=-72.59895940376188&format=json&addressdetails=1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000264188881C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "def get_zipcode(df, geolocator, lat_field, lon_field):\n",
    "\n",
    "  location = geolocator.reverse((df[lat_field], df[lon_field]), timeout= 1.5)\n",
    "  if 'state' in location.raw['address'] and df['state']!=location.raw['address']['state']:\n",
    "        df['state'] = location.raw['address']['state'] \n",
    "\n",
    "  if 'postcode' in location.raw['address'] and df['zip']!=location.raw['address']['postcode']:\n",
    "      df['zip'] = location.raw['address']['postcode']\n",
    "    \n",
    "  if 'county' in location.raw['address'] and df['county']!=location.raw['address']['county']:\n",
    "      df['county'] = location.raw['address']['county']\n",
    "    \n",
    "  if 'city' in location.raw['address'] and df['city']!=location.raw['address']['city']:\n",
    "      df['city'] = location.raw['address']['city']\n",
    "\n",
    "  return df\n",
    "\n",
    "geolocator = geopy.Nominatim(user_agent = 'myGeocode')\n",
    "dataframe = dataframe.apply(get_zipcode, axis=1, geolocator=geolocator, lat_field='latitude', lon_field='longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor,wait,ProcessPoolExecutor\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from threading import Thread\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocoding(dataframe):\n",
    "    print(\"started.\")\n",
    "    geolocator = geopy.Nominatim(user_agent = 'myGeocode',timeout=1.4)\n",
    "    def get_zipcode(df, geolocator, lat_field, lon_field):\n",
    "        location = geolocator.reverse((df[lat_field], df[lon_field]))\n",
    "        print(location['address'])\n",
    "        if 'state' in location.raw['address'] and df['state']!=location.raw['address']['state']:\n",
    "                df['state'] = location.raw['address']['state'] \n",
    "\n",
    "        if 'postcode' in location.raw['address'] and df['zip']!=location.raw['address']['postcode']:\n",
    "            df['zip'] = location.raw['address']['postcode']\n",
    "            \n",
    "        if 'county' in location.raw['address'] and df['county']!=location.raw['address']['county']:\n",
    "            df['county'] = location.raw['address']['county']\n",
    "            \n",
    "        if 'city' in location.raw['address'] and df['city']!=location.raw['address']['city']:\n",
    "            df['city'] = location.raw['address']['city']\n",
    "\n",
    "        return df\n",
    "    \n",
    "    dataframe = dataframe.apply(get_zipcode, axis=1, geolocator=geolocator, lat_field='latitude', lon_field='longitude')\n",
    "    print('Done')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0    given_name      surname   birthdate  \\\n",
      "0             0    Jacinto644      Kris249  2017-08-24   \n",
      "1             1       Alva958   Krajcik437  2016-08-01   \n",
      "2             2     Jayson808     Fadel536  1992-06-30   \n",
      "3             3      Jimmie93    Harris789  2004-01-09   \n",
      "4             4   Gregorio366       Auer97  1996-11-15   \n",
      "..          ...           ...          ...         ...   \n",
      "145         145     Josiah310    Stokes453  1987-01-24   \n",
      "146         146      Grady603  Hartmann983  1981-05-21   \n",
      "147         147  September423   Hermann103  2000-11-13   \n",
      "148         148       Altha90   Rolfson709  1999-08-16   \n",
      "149         149    Shandra823    Beatty507  2018-04-17   \n",
      "\n",
      "                            address             county         city  \\\n",
      "0         888 Hickle Ferry Suite 38     Hampden County     Chicopee   \n",
      "1               1048 Skiles Trailer     Norfolk County      Walpole   \n",
      "2         1056 Harris Lane Suite 70     Hampden County     Chicopee   \n",
      "3        201 Mitchell Lodge Unit 67    Plymouth County     Pembroke   \n",
      "4    1050 Lindgren Extension Apt 38     Suffolk County       Boston   \n",
      "..                              ...                ...          ...   \n",
      "145                 718 Bailey Wall   Middlesex County      Reading   \n",
      "146       376 Skiles Forge Suite 48   Worcester County  Westminster   \n",
      "147            243 Schamberger Club     Norfolk County      Norwood   \n",
      "148               606 O'Keefe Haven  Barnstable County   Barnstable   \n",
      "149        449 Cremin Estate Apt 20   Middlesex County   Burlington   \n",
      "\n",
      "             state    zip   latitude  longitude  \n",
      "0    Massachusetts  01013  42.151961 -72.598959  \n",
      "1    Massachusetts  02052  42.177370 -71.281353  \n",
      "2    Massachusetts  01040  42.181642 -72.608842  \n",
      "3    Massachusetts  02331  42.075292 -70.757035  \n",
      "4    Massachusetts  02210  42.352434 -71.028610  \n",
      "..             ...    ...        ...        ...  \n",
      "145  Massachusetts  01867  42.534820 -71.076579  \n",
      "146  Massachusetts  01473  42.533934 -71.935384  \n",
      "147  Massachusetts  02090  42.183472 -71.186825  \n",
      "148  Massachusetts  02672  41.625824 -70.331754  \n",
      "149  Massachusetts  01773  42.455734 -71.320955  \n",
      "\n",
      "[150 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 119\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y234sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# for data in dataframes:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y234sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#     process = Thread(target = geocoding, args = [data])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y234sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#     process.start()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y234sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# for process in futures:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y234sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#     process.join()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y234sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y234sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m executor\u001b[39m.\u001b[39mmap(geocoding,dataframes):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y234sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\concurrent\\futures\\process.py:484\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[0;32m    479\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[39m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[39m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[39m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m    485\u001b[0m         element\u001b[39m.\u001b[39mreverse()\n\u001b[0;32m    486\u001b[0m         \u001b[39mwhile\u001b[39;00m element:\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39;49mpop()\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39mpop()\u001b[39m.\u001b[39mresult(end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\concurrent\\futures\\_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    443\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\concurrent\\futures\\_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "# geolocator = geopy.Nominatim(user_agent = 'myGeocode',timeout=1.5)\n",
    "\n",
    "df1 = dataframe[0:50]\n",
    "df2 = dataframe[50:100]\n",
    "df3 = dataframe[100:150]\n",
    "# df4 = dataframe[150:200]\n",
    "# df5 = dataframe[200:250]\n",
    "\n",
    "dataframes = [df1,df2,df3]\n",
    "futures = []\n",
    "\n",
    "# for data in dataframes:\n",
    "#     process = Thread(target = geocoding, args = [data])\n",
    "#     process.start()\n",
    "#     futures.append(process)\n",
    "\n",
    "# for process in futures:\n",
    "#     process.join()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    \n",
    "    for result in executor.map(geocoding,dataframes):\n",
    "        print(result)\n",
    "    # df1 = executor.submit(geocoding,dataframe[0:100],geolocator)\n",
    "    # df2 = executor.submit(geocoding,dataframe[100:200],geolocator)\n",
    "    # df3 = executor.submit(geocoding,dataframe[200:300],geolocator)\n",
    "    # # # df5 = executor.submit(geocoding,dataframe[40:50],geolocator)\n",
    "\n",
    "\n",
    "    # print(df1.result())\n",
    "    # print(df2.result())\n",
    "    # print(df3.result())\n",
    " \n",
    "# df = pd.concat([df1.result(),df2.result(),df3.result()])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "from arcgis.geocoding import reverse_geocode\n",
    "\n",
    "gis = GIS(profile = \"your_online_profile\")\n",
    "results = reverse_geocode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(['Unnamed: 0'],axis = 1)\n",
    "lat_long= list(zip(latitude,longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long= list(zip(latitude,longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['latitude_longitude'] = lat_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent='myGeocoder')\n",
    "# location = geolocator.reverse((latitude,longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 117\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y221sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataframe[\u001b[39m'\u001b[39m\u001b[39mresult_geocoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataframe[\u001b[39m'\u001b[39;49m\u001b[39mlatitude_longitude\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(geolocator\u001b[39m.\u001b[39;49mreverse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1157\u001b[0m             values,\n\u001b[0;32m   1158\u001b[0m             f,\n\u001b[0;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1160\u001b[0m         )\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\geocoders\\nominatim.py:372\u001b[0m, in \u001b[0;36mNominatim.reverse\u001b[1;34m(self, query, exactly_one, timeout, language, addressdetails, zoom, namedetails)\u001b[0m\n\u001b[0;32m    370\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.reverse: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, url)\n\u001b[0;32m    371\u001b[0m callback \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_json, exactly_one\u001b[39m=\u001b[39mexactly_one)\n\u001b[1;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_geocoder(url, callback, timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\geocoders\\base.py:368\u001b[0m, in \u001b[0;36mGeocoder._call_geocoder\u001b[1;34m(self, url, callback, timeout, is_json, headers)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[39mif\u001b[39;00m is_json:\n\u001b[1;32m--> 368\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madapter\u001b[39m.\u001b[39;49mget_json(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mreq_headers)\n\u001b[0;32m    369\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter\u001b[39m.\u001b[39mget_text(url, timeout\u001b[39m=\u001b[39mtimeout, headers\u001b[39m=\u001b[39mreq_headers)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\adapters.py:447\u001b[0m, in \u001b[0;36mRequestsAdapter.get_json\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_json\u001b[39m(\u001b[39mself\u001b[39m, url, \u001b[39m*\u001b[39m, timeout, headers):\n\u001b[1;32m--> 447\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    448\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m         \u001b[39mreturn\u001b[39;00m resp\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\geopy\\adapters.py:457\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_request\u001b[39m(\u001b[39mself\u001b[39m, url, \u001b[39m*\u001b[39m, timeout, headers):\n\u001b[0;32m    456\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m         resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mget(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    458\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    459\u001b[0m         message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(error)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:542\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \n\u001b[0;32m    536\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 542\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    524\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    525\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[0;32m    526\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[0;32m    527\u001b[0m }\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    644\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    648\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    441\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    442\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    443\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    444\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    445\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    449\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    450\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    451\u001b[0m         )\n\u001b[0;32m    453\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\http\\client.py:1344\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1343\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1344\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1345\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\http\\client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    308\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    309\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\http\\client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 268\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    270\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataframe['result_geocoding'] = dataframe['latitude_longitude'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySQL Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mysql.connector\n",
    "from pymysql import connect\n",
    "from pandas.io import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(\n",
    "    host = \"localhost\",\n",
    "    user='root',\n",
    "    passwd=\"root\",\n",
    ")\n",
    "\n",
    "my_cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('information_schema',)\n",
      "('mysql',)\n",
      "('performance_schema',)\n",
      "('sakila',)\n",
      "('sys',)\n",
      "('world',)\n"
     ]
    }
   ],
   "source": [
    "query = \"show databases\"\n",
    "my_cursor.execute(query)\n",
    "databases = my_cursor.fetchall()\n",
    "\n",
    "for data in databases:\n",
    "    print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlalchemy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msqlalchemy\u001b[39;00m \u001b[39mimport\u001b[39;00m create_engine\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sqlalchemy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\AbhishekD6\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\compat\\_optional.py:141\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(name)\n\u001b[0;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\AbhishekD6\\AppData\\Local\\Programs\\Python\\Python38\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:973\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_duplicate \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39m'\u001b[39;49m\u001b[39m../../Dataset/Input Datasets/EHR_Linkage(2).xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\AbhishekD6\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\AbhishekD6\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\AbhishekD6\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    483\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\AbhishekD6\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1695\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m=\u001b[39m engine\n\u001b[0;32m   1693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage_options \u001b[39m=\u001b[39m storage_options\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engines[engine](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_io, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[1;32mc:\\Users\\AbhishekD6\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:556\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[1;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39m@doc\u001b[39m(storage_options\u001b[39m=\u001b[39m_shared_docs[\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    542\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    543\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    544\u001b[0m     filepath_or_buffer: FilePath \u001b[39m|\u001b[39m ReadBuffer[\u001b[39mbytes\u001b[39m],\n\u001b[0;32m    545\u001b[0m     storage_options: StorageOptions \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    546\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[39m    Reader using openpyxl engine.\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[39m    {storage_options}\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39;49m\u001b[39mopenpyxl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    557\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(filepath_or_buffer, storage_options\u001b[39m=\u001b[39mstorage_options)\n",
      "File \u001b[1;32mc:\\Users\\AbhishekD6\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\compat\\_optional.py:144\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 144\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m    145\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl."
     ]
    }
   ],
   "source": [
    "df_duplicate = pd.read_excel('../../Dataset/Input Datasets/EHR_Linkage(2).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['given_name', 'surname', 'birthdate', 'address', 'county', 'city',\n",
       "       'state', 'zip', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_duplicate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conn = create_engine(\"mysql+mysqldb://root:root@localhost/data_linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_duplicate.to_sql(con = my_conn,name='ehr_linkage_2', if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(\n",
    "    host = \"localhost\",\n",
    "    user='root',\n",
    "    passwd=\"root\",\n",
    "    db = 'data_linkage'\n",
    ")\n",
    "\n",
    "my_cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ehr_linkage_1',)\n",
      "('ehr_linkage_2',)\n",
      "('febrl_data4a',)\n",
      "('febrl_data4b',)\n"
     ]
    }
   ],
   "source": [
    "my_cursor.execute('show tables')\n",
    "tables = my_cursor.fetchall()\n",
    "for table in tables:\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cursor.execute('Drop Table febrl1_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linkage = pd.read_excel('../../Dataset/Input Datasets/EHR_Linkage(2).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'given_name', 'surname', 'birthdate', 'address', 'county',\n",
       "       'city', 'state', 'zip', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_linkage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12352"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_linkage = df_linkage[['given_name', 'surname', 'birthdate', 'address', 'county', 'city', 'state', 'zip', 'latitude', 'longitude']]\n",
    "len(df_linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conn = create_engine(\"mysql+mysqldb://root:root@localhost/data_linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GauravS15\\AppData\\Local\\Temp\\ipykernel_18120\\1643481361.py:1: UserWarning: The provided table name 'EHR_Deduplication' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  df_duplicate.to_sql(con = my_conn,name='EHR_Deduplication', if_exists='replace',index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_duplicate.to_sql(con = my_conn,name='EHR_Deduplication', if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GauravS15\\Documents\\Record Linkage 2.0\\Notebooks\\Main_Notebook\\Splink_.ipynb Cell 152\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y304sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conn \u001b[39m=\u001b[39m connect(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y304sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     host \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocalhost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y304sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     user\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y304sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     passwd\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y304sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     db \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdeduplication\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y304sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GauravS15/Documents/Record%20Linkage%202.0/Notebooks/Main_Notebook/Splink_.ipynb#Y304sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m my_cursor \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'connect' is not defined"
     ]
    }
   ],
   "source": [
    "conn = connect(\n",
    "    host = \"localhost\",\n",
    "    user='root',\n",
    "    passwd=\"root\",\n",
    "    db = 'deduplication'\n",
    ")\n",
    "\n",
    "my_cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ehr_deduplication',)\n",
      "('febrl1_data',)\n",
      "('febrl3_data',)\n"
     ]
    }
   ],
   "source": [
    "my_cursor.execute('show tables')\n",
    "tables = my_cursor.fetchall()\n",
    "for table in tables:\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my_cursor.execute('Drop Table febrl3_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
